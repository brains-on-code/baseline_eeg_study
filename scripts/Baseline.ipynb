{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9a9ba0-1fa4-467c-be6d-de2c92c50a30",
   "metadata": {},
   "source": [
    "# The Influence of Different Baselines on Program-Comprehension Studies\n",
    "## Partially from replication package (Peitek et al.), indicated by \"(from replication package)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b7d468",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94db508",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.version\n",
    "import sklearn\n",
    "from matplotlib.patches import Circle, Rectangle\n",
    "from scipy.integrate import simps\n",
    "import scipy.stats as stats\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import math\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import re\n",
    "import shutil\n",
    "from numpy import nan\n",
    "from cliffs_delta import cliffs_delta\n",
    "\n",
    "STYLES = [(0, (5, 1)),(0, (3, 1, 1, 1)),(0, (1, 1)),(0, (3, 1, 1, 1, 1, 1)),(0, ())]\n",
    "COLORS = [\"darkred\",\"darkorange\",\"darkcyan\",\"green\",\"y\"]\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "matplotlib.rcParams['mathtext.default']='regular'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"MNE: {mne.__version__}\")\n",
    "print(f\"Scipy: {scipy.__version__}\")\n",
    "print(f\"SKLearn: {sklearn.__version__}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create folder to save result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not os.path.exists('./data/RQ'):\n",
    "    os.makedirs('./data/RQ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to translate names used in data source to names used in paper\n",
    "def translate(s):\n",
    "    if s == \"PC\":\n",
    "        return \"ProgCompr\"\n",
    "    if s == \"Text\":\n",
    "        return \"Read\"\n",
    "    if s == \"Rest\":\n",
    "        return \"CrossFix\"\n",
    "    if s == \"Matrix\":\n",
    "        return \"ProbSolv\"\n",
    "    return s\n",
    "\n",
    "# Function to convert a string seperated by whitespace characters back to python list (from replication package)\n",
    "def string_to_list_string(data):\n",
    "    data = data.replace(' ', ',')\n",
    "    data = data.replace('\\n', ',')\n",
    "    data = ','.join([element for element in data.split(\",\") if len(element) > 0])\n",
    "    if data[1] == \",\":\n",
    "        data = \"[\" + data[2:]\n",
    "    return data\n",
    "\n",
    "# (from replication package)\n",
    "def get_spectrum(data, sampling_rate, method='welch', decibel=False, resolution='auto'):\n",
    "    \"\"\"\n",
    "    Calculate amplitude or power spectrum\n",
    "\n",
    "    data: Should be of shape (n_channels, n_samples)\n",
    "    sampling_rate: Sampling rate... (float)\n",
    "    method:\n",
    "        * welch for power spectrum using Welch's method (recommended)\n",
    "        * ft for simple Fourier transform (amplitude spectrum)\n",
    "        * ps for power spectrum using a simple fourier transform\n",
    "    decibel: Convert spectrum to decibel (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    axis = -1\n",
    "\n",
    "    n_channels, n_samples = data.shape\n",
    "\n",
    "    if resolution == 'auto':\n",
    "        n_frequencies = n_samples\n",
    "    elif isinstance(resolution, (int, float)):\n",
    "        n_frequencies = np.round(sampling_rate / resolution).astype('int')\n",
    "    else:\n",
    "        raise ValueError('\\'{}\\''.format(resolution))\n",
    "\n",
    "    # Spectrum\n",
    "    if method in ['ft', 'ps']:\n",
    "        # Using FFT\n",
    "        # Get (complex) spectrum\n",
    "        spec = np.fft.fft(data, n=n_frequencies, axis=axis)\n",
    "        freq = np.fft.fftfreq(n_frequencies) * sampling_rate\n",
    "\n",
    "        # Convert to real positive-sided spectrum\n",
    "        spec = np.abs(spec)\n",
    "        nyquist = 0.5 * sampling_rate\n",
    "        is_positive = np.logical_or(np.logical_and(freq >= 0, freq <= nyquist), freq == -nyquist)\n",
    "        n_pos = np.sum(is_positive)\n",
    "        is_positive = np.repeat(is_positive[np.newaxis], n_channels, axis=0)\n",
    "        spec = np.reshape(spec[is_positive], (n_channels, n_pos))\n",
    "        freq = np.abs(freq[is_positive])\n",
    "        is_double = np.logical_and(freq > 0, freq < nyquist)\n",
    "        is_double = np.repeat(is_double[np.newaxis], n_channels, axis=0)\n",
    "        spec[is_double] = 2 * spec[is_double]\n",
    "\n",
    "        if method in ['ps']:\n",
    "            # Get power spectral density\n",
    "            spec = (1 / (sampling_rate * n_frequencies)) * spec ** 2\n",
    "\n",
    "            # Convert to decibel if required\n",
    "            if decibel:\n",
    "                spec = _to_decibel(spec)\n",
    "    elif method in ['welch', 'welch_db']:\n",
    "        # Using Welch method\n",
    "        freq, spec = signal.welch(data, sampling_rate, nperseg=n_frequencies, detrend='constant', axis=axis)\n",
    "\n",
    "        # Convert to decibel if required\n",
    "        if decibel:\n",
    "            spec = _to_decibel(spec)\n",
    "    else:\n",
    "        raise RuntimeError('Unknown method \\'{}\\''.format(method))\n",
    "\n",
    "    return spec, freq\n",
    "\n",
    "# (from replication package)\n",
    "def bandpower(spec, freq, freqband, relative=False):\n",
    "    \"\"\"\n",
    "    Get band power within specified frequency band\n",
    "    Alternatively: https://raphaelvallat.com/bandpower.html\n",
    "    \"\"\"\n",
    "\n",
    "    spec = np.asarray(spec)\n",
    "    freq = np.asarray(freq)\n",
    "    freqband = np.asarray(freqband)\n",
    "\n",
    "    if spec.ndim != 1:\n",
    "        raise ValueError('Input \\'spec\\' bad: {}'.format(spec.shape))\n",
    "\n",
    "    if freqband.ndim != 1 and freqband.shape[-1] != 2:\n",
    "        raise ValueError('Input \\'freqband\\' bad: {}'.format(freqband.shape))\n",
    "\n",
    "    # Frequency resolution\n",
    "    step_freq = freq[1] - freq[0]\n",
    "\n",
    "    # Find closest indices of band in frequency vector\n",
    "    is_in_freqband = np.logical_and(freq >= np.min(freqband), freq <= np.max(freqband))\n",
    "\n",
    "    # Integral approximation of the spectrum using Simpson's rule\n",
    "    bp = simps(spec[is_in_freqband], dx=step_freq)\n",
    "\n",
    "    if relative:\n",
    "        bp = bp / simps(spec, dx=step_freq)\n",
    "\n",
    "    return bp\n",
    "\n",
    "# (from replication package)\n",
    "def _to_decibel(spec):\n",
    "    return 10 * np.log10(spec)\n",
    "\n",
    "\n",
    "# Function to calculate mental work load from eeg data (from replication package)\n",
    "def get_mental_work_load(eeg_path, sampling_rate=500):\n",
    "    # read in eeg file\n",
    "    eeg_data = mne.io.read_raw_fif(eeg_path, preload=True, verbose='ERROR')\n",
    "\n",
    "    # get raw channel data and do mean average referencing\n",
    "    eeg_data_raw = eeg_data.get_data()\n",
    "    eeg_data_ref = eeg_data_raw - np.mean(eeg_data_raw, axis=0)\n",
    "\n",
    "    # extract channel names of eeg_data\n",
    "    channel_names = list(eeg_data.to_data_frame().columns[1:])\n",
    "\n",
    "    # create mock events for cutting eeg data (number, len, id)\n",
    "    # events are set every 50 entry to move the window 0.1 seconds every time\n",
    "    events = np.array([(i, 0, 1) for i in range(0, eeg_data_ref.shape[-1], 50)])\n",
    "\n",
    "    # create temporal eeg raw for cutting data into epochs\n",
    "    tmp_raw = mne.io.RawArray(eeg_data_ref, eeg_data.info, verbose='ERROR')\n",
    "\n",
    "    # create epochs which have a 3 second window and operate on event id 1\n",
    "    epochs = mne.epochs.Epochs(tmp_raw, events, event_id=1, tmin=0, tmax=3, baseline=None, preload=True,\n",
    "                               verbose='ERROR')\n",
    "\n",
    "    # calculate Mental Workload for each window\n",
    "    mwl_array = []\n",
    "\n",
    "    for epoch_data_raw in epochs:\n",
    "        # perform power spectrum analysis on eeg data\n",
    "        spectrum, frequency = get_spectrum(epoch_data_raw, sampling_rate, method='welch', decibel=False,\n",
    "                                           resolution='auto')\n",
    "\n",
    "        # calculate Mental Workload by dividing the power spectrum of the relative theta band power divided by relative alpha band power\n",
    "        # on channel Fz for theta and Pc for alpha\n",
    "        theata_power = bandpower(spectrum[channel_names.index(\"Fz\"), :], frequency, [4.0, 8.0], relative=True)\n",
    "        alpha_power = bandpower(spectrum[channel_names.index(\"Pz\"), :], frequency, [8.0, 13.0], relative=True)\n",
    "        MWL = theata_power / alpha_power\n",
    "\n",
    "        # append Mental Workload to array\n",
    "        mwl_array.append(MWL)\n",
    "    return mwl_array, len(tmp_raw), len(epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EEG"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_filtered = pd.read_csv(f\"./data/filteredData/filtered_data.csv\")\n",
    "df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate Mental Workload for each participant for each algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sampling rate of the EEG data\n",
    "sampling_rate = 500\n",
    "\n",
    "# create Mental Workload column\n",
    "df_filtered[\"MentalWorkLoad_Program\"] = [np.array([]) for i in range(len(df_filtered))]\n",
    "df_filtered[\"MentalWorkLoad_Baseline\"] = [np.array([]) for i in range(len(df_filtered))]\n",
    "df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"] = [np.array([]) for i in range(len(df_filtered))]\n",
    "df_filtered[\"MentalWorkLoad_Seconds_Per_Sample\"] = [np.array([]) for i in range(len(df_filtered))]\n",
    "\n",
    "# iterate over each row anc calculate Mental Workload for the task\n",
    "for idx in tqdm(range(len(df_filtered))):\n",
    "    program_eeg_path = df_filtered.iloc[idx][\"ProgramEEG\"]\n",
    "    baseline_eeg_path = df_filtered.iloc[idx][\"BaselineEEG\"]\n",
    "\n",
    "    # calculate Mental Workload for each algorithm (from replication package)\n",
    "    mwl_program, data_len, mwl_len = get_mental_work_load(program_eeg_path)\n",
    "\n",
    "    # calculate Mental Workload for the baseline task (from replication package)\n",
    "    mwl_baseline, data_len_b, mwl_len_b = get_mental_work_load(baseline_eeg_path)\n",
    "\n",
    "    # calculate Mental Workload baseline corrected\n",
    "    baseline_mean = np.mean(mwl_baseline)\n",
    "    mwl_program_minus_baseline = mwl_program - baseline_mean\n",
    "\n",
    "    # Calculate sample duration in seconds\n",
    "    try:\n",
    "        data_per_mwl_sample = data_len / mwl_len\n",
    "        seconds_per_mwl_sample = data_per_mwl_sample / sampling_rate\n",
    "    except:\n",
    "        seconds_per_mwl_sample = None\n",
    "\n",
    "    # append Mental Workload to the current row\n",
    "    df_filtered.at[idx, \"MentalWorkLoad_Program\"] = mwl_program\n",
    "    df_filtered.at[idx, \"MentalWorkLoad_Baseline\"] = mwl_baseline\n",
    "    df_filtered.at[idx, \"MentalWorkLoad_Program_Minus_Baseline\"] = mwl_program_minus_baseline\n",
    "    df_filtered.at[idx, \"MentalWorkLoad_Seconds_Per_Sample\"] = seconds_per_mwl_sample\n",
    "df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate answer times"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_filtered[\"BaselineAnswerTime\"] = -1\n",
    "df_filtered[\"ProgramAnswerTime\"] = -1\n",
    "\n",
    "# Iterate over all tasks\n",
    "for idx in tqdm(range(len(df_filtered))):\n",
    "    df_filtered.at[idx, \"BaselineAnswerTime\"] = df_filtered.iloc[idx][\"BaselineEndTime\"] - df_filtered.iloc[idx][\"BaselineStartTime\"]\n",
    "    df_filtered.at[idx, \"ProgramAnswerTime\"] = df_filtered.iloc[idx][\"ProgramEndTime\"] - df_filtered.iloc[idx][\"ProgramStartTime\"]\n",
    "\n",
    "df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checkpoint: Save calculated data (from replication package)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the filtered dataframe to csv\n",
    "df_tmp = df_filtered.copy()\n",
    "df_tmp[\"MentalWorkLoad_Program\"] = df_tmp[\"MentalWorkLoad_Program\"].apply(lambda series: str(list(series)))\n",
    "df_tmp[\"MentalWorkLoad_Baseline\"] = df_tmp[\"MentalWorkLoad_Baseline\"].apply(lambda series: str(list(series)))\n",
    "df_tmp[\"MentalWorkLoad_Program_Minus_Baseline\"] = df_tmp[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(lambda series: str(list(series)))\n",
    "df_tmp.to_csv(\"./data/RQ/MentalWorkLoadRaw.csv\", sep=\";\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load saved data (from replication package)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_filtered = pd.read_csv(\"./data/RQ/MentalWorkLoadRaw.csv\", sep=\";\")\n",
    "df_filtered[\"MentalWorkLoad_Program\"] = df_filtered[\"MentalWorkLoad_Program\"].apply(string_to_list_string)\n",
    "df_filtered[\"MentalWorkLoad_Baseline\"] = df_filtered[\"MentalWorkLoad_Baseline\"].apply(string_to_list_string)\n",
    "df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"] = df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(string_to_list_string)\n",
    "df_filtered[\"MentalWorkLoad_Program\"] = df_filtered[\"MentalWorkLoad_Program\"].apply(lambda x: np.array(eval(x)))\n",
    "df_filtered[\"MentalWorkLoad_Baseline\"] = df_filtered[\"MentalWorkLoad_Baseline\"].apply(lambda x: np.array(eval(x)))\n",
    "df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"] = df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(lambda x: np.array(eval(x)))\n",
    "df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outlier removal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Drop MentalWorkLoad NaN\n",
    "Because answer time was shorter than 3 seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "before = len(df_filtered)\n",
    "df_filtered = df_filtered.drop(df_filtered[df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(len).eq(0)].index)\n",
    "df_filtered = df_filtered.drop(df_filtered[df_filtered[\"MentalWorkLoad_Baseline\"].apply(len).eq(0)].index)\n",
    "df_filtered = df_filtered.drop(df_filtered[df_filtered[\"MentalWorkLoad_Program\"].apply(len).eq(0)].index)\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "print(\"Dropped \" + str(before-len(df_filtered)) + \" rows\")\n",
    "df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Drop skipped"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "before = len(df_filtered)\n",
    "df_droped_because_skipped_baseline = df_filtered[df_filtered[\"BaselineSkipped\"] == True].copy()\n",
    "df_droped_because_skipped_program = df_filtered[df_filtered[\"ProgramSkipped\"] == True].copy()\n",
    "df_filtered = df_filtered.drop(df_filtered[df_filtered[\"BaselineSkipped\"] == True].index)\n",
    "df_filtered = df_filtered.drop(df_filtered[df_filtered[\"ProgramSkipped\"] == True].index)\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "print(\"Dropped \" + str(before-len(df_filtered)) + \" rows\")\n",
    "df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save rows dropped because skipped"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_droped_because_skipped_baseline.to_csv(\"./data/RQ/droped_because_skipped_baseline.csv\", sep=\";\", index=False)\n",
    "df_droped_because_skipped_baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_droped_because_skipped_program.to_csv(\"./data/RQ/droped_because_skipped_program.csv\", sep=\";\", index=False)\n",
    "df_droped_because_skipped_program"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Drop outlier\n",
    "Datapoints with answer time outside 1.5 times the interquartile range are dropped."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df_task_quartiles = pd.DataFrame(columns = [\"Task\", \"Q1\", \"Q2\", \"Q3\", \"Minimum\", \"Maximum\"])\n",
    "df_droped_because_outlier = pd.DataFrame(columns=df_filtered.columns)\n",
    "df_droped_because_outlier[\"Cause\"] = \"\"\n",
    "\n",
    "before = len(df_filtered)\n",
    "\n",
    "print(\"Calculate and drop outlier for algorithms\")\n",
    "for algorithm in tqdm(df_filtered[\"Algorithm\"].unique()):\n",
    "    df_algo = df_filtered[df_filtered[\"Algorithm\"] == algorithm]\n",
    "\n",
    "    # Calculate interquartile range for task\n",
    "    q1, q2, q3 = np.percentile(df_algo[\"ProgramAnswerTime\"], [25,50,75])\n",
    "    minimum = q1 - 1.5*(q3-q1)\n",
    "    maximum = q3 + 1.5*(q3-q1)\n",
    "\n",
    "    # Save calculated range in dataframe\n",
    "    df_task_quartiles = pd.concat([df_task_quartiles, pd.DataFrame(data={\"Task\": [algorithm], \"Q1\": [q1], \"Q2\": [q2], \"Q3\": [q3], \"Minimum\": [minimum], \"Maximum\": [maximum]})],ignore_index=True)\n",
    "\n",
    "    for index, row in df_algo.iterrows():\n",
    "        if (row[\"ProgramAnswerTime\"] < minimum or row[\"ProgramAnswerTime\"] > maximum):\n",
    "            # Drop row, if it don't matches range\n",
    "            df_droped_because_outlier.loc[len(df_droped_because_outlier)] = row.copy()\n",
    "            df_droped_because_outlier.loc[len(df_droped_because_outlier)-1][\"Cause\"] = \"PC\"\n",
    "            df_filtered = df_filtered.drop(index)\n",
    "\n",
    "print(\"Calculate and drop outlier for baseline tasks\")\n",
    "for baseline_task in tqdm(df_filtered[\"BaselineTask\"].unique()):\n",
    "    df_baseline_task = df_filtered[df_filtered[\"BaselineTask\"] == baseline_task]\n",
    "\n",
    "    # Calculate interquartile range for task\n",
    "    q1, q2, q3 = np.percentile(df_baseline_task[\"BaselineAnswerTime\"], [25,50,75])\n",
    "    minimum = q1 - 1.5*(q3-q1)\n",
    "    maximum = q3 + 1.5*(q3-q1)\n",
    "\n",
    "    # Save calculated range in dataframe\n",
    "    df_task_quartiles = pd.concat([df_task_quartiles, pd.DataFrame(data={\"Task\": [baseline_task], \"Q1\": [q1], \"Q2\": [q2], \"Q3\": [q3], \"Minimum\": [minimum], \"Maximum\": [maximum]})],ignore_index=True)\n",
    "\n",
    "    for index, row in df_baseline_task.iterrows():\n",
    "        if (row[\"BaselineAnswerTime\"] < minimum or row[\"BaselineAnswerTime\"] > maximum):\n",
    "            # Drop row, if it don't matches range\n",
    "            df_droped_because_outlier.loc[len(df_droped_because_outlier)] = row.copy()\n",
    "            df_droped_because_outlier.loc[len(df_droped_because_outlier)-1][\"Cause\"] = \"Baseline\"\n",
    "            df_filtered = df_filtered.drop(index)\n",
    "\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "print(\"Dropped \" + str(before-len(df_filtered)) + \" rows\")\n",
    "\n",
    "# Save quartiles and range for each task\n",
    "df_task_quartiles.to_csv(\"./data/RQ/task_quartiles.csv\", sep=\";\", index=False)\n",
    "df_task_quartiles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save rows dropped because outlier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_droped_because_outlier.to_csv(\"./data/RQ/droped_because_outlier.csv\", sep=\";\", index=False)\n",
    "df_droped_because_outlier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save dataframe for creating topoplots\n",
    "Subset of df_filtered with only the needed columns for topoplot creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./data/RQ/topo_data\"):\n",
    "    os.makedirs(\"./data/RQ/topo_data\")\n",
    "df_topo = df_filtered[[\"Participant\", \"Algorithm\", \"Baseline\", \"ProgramEEG\", \"BaselineEEG\"]]\n",
    "df_topo.to_csv(\"./data/RQ/topo_data/data.csv\", sep=\";\", index=False)\n",
    "df_topo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate remaining rows per participant"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df_participant_rows = pd.DataFrame(columns = [\"Participant\", \"Rows\"])\n",
    "\n",
    "# Add rows per participant\n",
    "for participant in tqdm(df_filtered[\"Participant\"].unique()):\n",
    "    df_participant = df_filtered[df_filtered[\"Participant\"] == participant]\n",
    "    df_participant_rows = pd.concat([df_participant_rows, pd.DataFrame(data={\"Participant\": [participant], \"Rows\": [len(df_participant)]})],ignore_index=True)\n",
    "\n",
    "# Sort and save dataframe\n",
    "df_participant_rows.sort_values(\"Rows\", inplace=True, ignore_index=True)\n",
    "df_participant_rows.to_csv(\"./data/RQ/participant_rows.csv\", sep=\";\", index=False)\n",
    "df_participant_rows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**\n",
    "Run script until here, then exclude whole participants (<50%) and then run again, whithout them for further analysis. (Security feature, that no participant is \"accidently\" removed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate metrics and stats for MWL (from replication package)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# stats per algorithm per participant\n",
    "filter = df_filtered.filter(\n",
    "    [\"MWLLength\", \"MWLMean\", \"MWLMedian\", \"MWLStd\", \"MWLMin\", \"MWLMax\", \"MWL_average_slope\", \"MWL_total_slope\",\n",
    "     \"PeakToPeak\", \"MWL_max_slope\"])\n",
    "df_filtered = df_filtered.drop(filter, axis=1)\n",
    "\n",
    "df_filtered.insert(loc=0, column=\"MWLLength\", value=df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(len))\n",
    "df_filtered.insert(loc=0, column=\"MWLMean\", value=df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(np.mean))\n",
    "df_filtered.insert(loc=0, column=\"MWL_Program_Mean\", value=df_filtered[\"MentalWorkLoad_Program\"].apply(np.mean))\n",
    "df_filtered.insert(loc=0, column=\"MWL_Baseline_Mean\", value=df_filtered[\"MentalWorkLoad_Baseline\"].apply(np.mean))\n",
    "df_filtered.insert(loc=0, column=\"MWLMedian\", value=df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(np.median))\n",
    "df_filtered.insert(loc=0, column=\"MWLStd\", value=df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(np.std))\n",
    "df_filtered.insert(loc=0, column=\"MWLMin\", value=df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(np.min))\n",
    "df_filtered.insert(loc=0, column=\"MWLMax\", value=df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(np.max))\n",
    "df_filtered.insert(loc=0, column=\"PeakToPeak\", value=df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(np.ptp))\n",
    "df_filtered.insert(loc=0, column=\"MWL_max_slope\",\n",
    "                   value=df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(lambda x: np.amax(np.diff(x))))\n",
    "df_filtered.insert(loc=0, column=\"MWL_average_slope\",\n",
    "                   value=df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(lambda x: np.mean(np.diff(x))))\n",
    "df_filtered.insert(loc=0, column=\"MWL_total_slope\",\n",
    "                   value=df_filtered[\"MentalWorkLoad_Program_Minus_Baseline\"].apply(lambda x: np.sum(np.diff(x))))\n",
    "\n",
    "df_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save metrics and stats for MWL to csv for further analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_filtered[[\"MWL_total_slope\", \"MWL_average_slope\", \"MWLMax\", \"MWLMin\", \"MWLStd\", \"MWLMedian\", \"MWLMean\", \"MWL_Program_Mean\", \"MWL_Baseline_Mean\", \"MWLLength\",\n",
    "             \"Participant\", \"Algorithm\"]].to_csv(\"./data/RQ/MentalWorkLoadStatsRaw.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate answer time and correctness statistics per task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df_task_statistics = pd.DataFrame(columns = [\"Task\", \"AnswerTime_Minimum\",\"AnswerTime_Maximum\", \"AnswerTime_Mean\", \"Correctness\"])\n",
    "\n",
    "for task in df_filtered[\"BaselineTask\"].unique():\n",
    "    df_task = df_filtered[df_filtered[\"BaselineTask\"] == task]\n",
    "    df_correct_task = df_task[df_task[\"BaselineCorrect\"] == True]\n",
    "\n",
    "    # Add to dataframe\n",
    "    df_task_statistics = pd.concat([df_task_statistics, pd.DataFrame(data={\"Task\": task, \"AnswerTime_Minimum\": [df_task[\"BaselineAnswerTime\"].min()], \"AnswerTime_Maximum\": [df_task[\"BaselineAnswerTime\"].max()], \"AnswerTime_Mean\": [df_task[\"BaselineAnswerTime\"].mean()], \"Correctness\": [len(df_correct_task)/len(df_task)]})],ignore_index=True)\n",
    "\n",
    "for task in df_filtered[\"Algorithm\"].unique():\n",
    "    df_task = df_filtered[df_filtered[\"Algorithm\"] == task]\n",
    "    df_correct_task = df_task[df_task[\"ProgramCorrect\"] == True]\n",
    "\n",
    "    # Add to dataframe\n",
    "    df_task_statistics = pd.concat([df_task_statistics, pd.DataFrame(data={\"Task\": task, \"AnswerTime_Minimum\": [df_task[\"ProgramAnswerTime\"].min()], \"AnswerTime_Maximum\": [df_task[\"ProgramAnswerTime\"].max()], \"AnswerTime_Mean\": [df_task[\"ProgramAnswerTime\"].mean()], \"Correctness\": [len(df_correct_task)/len(df_task)]})],ignore_index=True)\n",
    "\n",
    "df_task_statistics.to_csv(\"./data/RQ/task_statistics.csv\", sep=\";\", index=False)\n",
    "df_task_statistics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate length information for averaging mental work load\n",
    "\n",
    "**Note**\n",
    "For calculating the average we only take the sample slice from zero to the mean lenght of all datapoints of the same type (Program comprehension, Math, ...) into account. This means, that we have minimum 50% of the data for calculating an average."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df_information = pd.DataFrame(columns = [\"Type\", \"MedianLength\",\"SampleCount\", \"MedianLength_PC\", \"SampleCount_PC\", \"75\", \"75_PC\"])\n",
    "length_pc_total = []\n",
    "\n",
    "# Iterate over all baselines\n",
    "for baseline in df_filtered[\"Baseline\"].unique():\n",
    "    df_baseline = df_filtered[df_filtered[\"Baseline\"] == baseline]\n",
    "    length = []\n",
    "    length_pc = []\n",
    "\n",
    "    # Add length for baseline and program comprehension\n",
    "    for index, row in df_baseline.iterrows():\n",
    "        length.append(len(row[\"MentalWorkLoad_Baseline\"]))\n",
    "        length_pc.append(len(row[\"MentalWorkLoad_Program\"]))\n",
    "        length_pc_total.append(len(row[\"MentalWorkLoad_Program\"]))\n",
    "\n",
    "    # Calculate median for baseline\n",
    "    median_length = math.floor(np.median(length))\n",
    "\n",
    "    # Count samples for each datapoint for baseline\n",
    "    sample_count = [0] * median_length\n",
    "    for l in length:\n",
    "        for i in range(min(median_length, l)):\n",
    "            sample_count[i] += 1\n",
    "\n",
    "    # Calculate 75% marker for baseline\n",
    "    marker_75_baseline = -1\n",
    "    for i in range(len(sample_count)):\n",
    "        if sample_count[i] / sample_count[0] >= 0.75:\n",
    "            marker_75_baseline = i\n",
    "    if marker_75_baseline == len(sample_count)-1:\n",
    "        marker_75_baseline = -1\n",
    "\n",
    "    # Calculate median for program comprehension\n",
    "    median_length_pc = math.floor(np.median(length_pc))\n",
    "\n",
    "    # Count samples for each datapoint for program comprehension\n",
    "    sample_count_pc = [0] * median_length_pc\n",
    "    for l in length_pc:\n",
    "        for i in range(min(median_length_pc, l)):\n",
    "            sample_count_pc[i] += 1\n",
    "\n",
    "    # Calculate 75% marker for program comprehension\n",
    "    marker_75_program = -1\n",
    "    for i in range(len(sample_count_pc)):\n",
    "        if sample_count_pc[i] / sample_count_pc[0] >= 0.75:\n",
    "            marker_75_program = i\n",
    "    if marker_75_program == len(sample_count_pc)-1:\n",
    "        marker_75_program = -1\n",
    "\n",
    "    # Add to dataframe\n",
    "    df_information = pd.concat([df_information, pd.DataFrame(data={\"Type\": baseline, \"MedianLength\": median_length, \"SampleCount\": [sample_count], \"MedianLength_PC\": median_length_pc, \"SampleCount_PC\": [sample_count_pc], \"75\": [marker_75_baseline], \"75_PC\": [marker_75_program]})],ignore_index=True)\n",
    "\n",
    "# Calculate median for program comprehension\n",
    "median_length_pc_total = math.floor(np.median(length_pc_total))\n",
    "\n",
    "# Count samples for each datapoint for program comprehension\n",
    "sample_count_pc_total = [0] * median_length_pc_total\n",
    "for l in length_pc_total:\n",
    "    for i in range(min(median_length_pc_total, l)):\n",
    "        sample_count_pc_total[i] += 1\n",
    "\n",
    "# Calculate 75% marker for program comprehension\n",
    "marker_75_program_total = -1\n",
    "for i in range(len(sample_count_pc_total)):\n",
    "    if sample_count_pc_total[i] / sample_count_pc_total[0] >= 0.75:\n",
    "        marker_75_program_total = i\n",
    "if marker_75_program_total == len(sample_count_pc_total)-1:\n",
    "    marker_75_program_total = -1\n",
    "\n",
    "# Add to dataframe\n",
    "df_information = pd.concat([df_information, pd.DataFrame(data={\"Type\": \"Program\", \"MedianLength\": median_length_pc_total, \"SampleCount\": [sample_count_pc_total], \"75\": [marker_75_program_total]})],ignore_index=True)\n",
    "\n",
    "seconds_per_sample_mean = df_filtered[\"MentalWorkLoad_Seconds_Per_Sample\"].mean()\n",
    "seconds_per_sample_round = round(seconds_per_sample_mean, 4)\n",
    "samples_per_second = 1/seconds_per_sample_mean\n",
    "\n",
    "df_information"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate and plot average per Baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/plots/averaged_total\"):\n",
    "    os.makedirs(\"./data/RQ/plots/averaged_total\")\n",
    "if not os.path.exists(\"./data/RQ/plots/averaged_program_comprehension_with_baseline\"):\n",
    "    os.makedirs(\"./data/RQ/plots/averaged_program_comprehension_with_baseline\")\n",
    "\n",
    "# Create dataframe\n",
    "df_averaged_over_all = pd.DataFrame(columns = [\"Type\", \"MWL\",\"MWL_Mean\"])\n",
    "df_averaged_program_comprehension_with_baseline = pd.DataFrame(columns = [\"Baseline\", \"MWL\",\"MWL_Minus_Baseline\"])\n",
    "\n",
    "# Init vars for program comprehension calculation\n",
    "program_median_length = df_information[df_information['Type'] == \"Program\"][\"MedianLength\"].item()\n",
    "program_sample_count = df_information[df_information['Type'] == \"Program\"][\"SampleCount\"].item()\n",
    "program_marker_75 = df_information[df_information['Type'] == \"Program\"][\"75\"].item()\n",
    "program_averaged = [0] * program_median_length\n",
    "\n",
    "total_plot = []\n",
    "total_baseline_plot = []\n",
    "total_program_split_by_baseline_plot = []\n",
    "\n",
    "total_detail_fig, total_detail_axs = plt.subplots(1, 4, sharex='all', sharey='all',figsize=(20,5))\n",
    "count = 0\n",
    "# Iterate over all baselines\n",
    "baselines = df_filtered[\"Baseline\"].unique()\n",
    "for baseline in tqdm(baselines):\n",
    "    # Init vars for baseline calculation\n",
    "    df_baseline = df_filtered[df_filtered[\"Baseline\"] == baseline]\n",
    "    baseline_median_length = df_information[df_information['Type'] == baseline][\"MedianLength\"].item()\n",
    "    baseline_sample_count = df_information[df_information['Type'] == baseline][\"SampleCount\"].item()\n",
    "    baseline_marker_75 = df_information[df_information['Type'] == baseline][\"75\"].item()\n",
    "    baseline_median_length_pc = df_information[df_information['Type'] == baseline][\"MedianLength_PC\"].item()\n",
    "    baseline_sample_count_pc = df_information[df_information['Type'] == baseline][\"SampleCount_PC\"].item()\n",
    "    baseline_marker_75_pc = df_information[df_information['Type'] == baseline][\"75_PC\"].item()\n",
    "    baseline_averaged = [0] * baseline_median_length\n",
    "    program_per_baseline_averaged = [0] * baseline_median_length_pc\n",
    "    program_per_baseline_averaged_minus_baseline = [0] * baseline_median_length_pc\n",
    "\n",
    "    for index, row in df_baseline.iterrows():\n",
    "        mwl_base = row[\"MentalWorkLoad_Baseline\"]\n",
    "        mwl_program = row[\"MentalWorkLoad_Program\"]\n",
    "        mwl_program_minus_baseline = row[\"MentalWorkLoad_Program_Minus_Baseline\"]\n",
    "        # Add baseline mental work load\n",
    "        for i in range(0, min(len(mwl_base), baseline_median_length)):\n",
    "            baseline_averaged[i] += mwl_base[i]\n",
    "        # Add program mental work load\n",
    "        for i in range(0, min(len(mwl_program), program_median_length)):\n",
    "            program_averaged[i] += mwl_program[i]\n",
    "        # Add program mental work load with baseline\n",
    "        for i in range(0, min(len(mwl_program), baseline_median_length_pc)):\n",
    "            program_per_baseline_averaged[i] += mwl_program[i]\n",
    "        # Add program minus baseline mental work load with baseline\n",
    "        for i in range(0, min(len(mwl_program_minus_baseline), baseline_median_length_pc)):\n",
    "            program_per_baseline_averaged_minus_baseline[i] += mwl_program_minus_baseline[i]\n",
    "\n",
    "    # Average baseline mental work load\n",
    "    for i in range(0, len(baseline_averaged)):\n",
    "        baseline_averaged[i] = baseline_averaged[i] / baseline_sample_count[i]\n",
    "    df_averaged_over_all = pd.concat([df_averaged_over_all, pd.DataFrame(data={\"Type\": translate(baseline), \"MWL\": [baseline_averaged], \"MWL_Mean\": np.mean(baseline_averaged)})],ignore_index=True)\n",
    "\n",
    "    # Average program mental work load\n",
    "    for i in range(0, len(program_per_baseline_averaged)):\n",
    "        program_per_baseline_averaged[i] = program_per_baseline_averaged[i] / baseline_sample_count_pc[i]\n",
    "        program_per_baseline_averaged_minus_baseline[i] = program_per_baseline_averaged_minus_baseline[i] / baseline_sample_count_pc[i]\n",
    "    df_averaged_program_comprehension_with_baseline = pd.concat([df_averaged_program_comprehension_with_baseline, pd.DataFrame(data={\"Baseline\": translate(baseline), \"MWL\": [program_per_baseline_averaged], \"MWL_Minus_Baseline\": [program_per_baseline_averaged_minus_baseline]})],ignore_index=True)\n",
    "\n",
    "    total_plot.append((program_per_baseline_averaged_minus_baseline, f\"$ProgCompr_{{{translate(baseline)}}}$ > {translate(baseline)}\"))\n",
    "    total_baseline_plot.append((baseline_averaged, f\"{translate(baseline)}\"))\n",
    "    total_program_split_by_baseline_plot.append((program_per_baseline_averaged, f\"{translate(baseline)}\"))\n",
    "\n",
    "    # Plot averaged baseline mental work load and mean\n",
    "    style_it=itertools.cycle(STYLES)\n",
    "    color_it=itertools.cycle(COLORS)\n",
    "    plt.figure()\n",
    "    color=next(color_it)\n",
    "    linestyle=next(style_it)\n",
    "    p = plt.plot(baseline_averaged, color=color, linestyle=linestyle, label=f\"{translate(baseline)}\")\n",
    "    plt.axhline(y=np.mean(baseline_averaged), color=color, linestyle=linestyle, label=f\"{translate(baseline)} Mean\")\n",
    "    if baseline_marker_75 > 0:\n",
    "        plt.axvline(x=baseline_marker_75, linestyle='solid', color='k')\n",
    "        plt.text(baseline_marker_75,0.01,'75% ', ha='right', va='bottom', transform=plt.gca().get_xaxis_transform())\n",
    "    plt.legend(handlelength=3)\n",
    "    plt.xlabel(f\"Seconds\")\n",
    "    plt.ylabel(\"Mental Load\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "    # Convert samples in seconds\n",
    "    locs, labels = plt.xticks()\n",
    "    labels_seconds = []\n",
    "    for label in labels:\n",
    "        labels_seconds.append(str(round(float(re.sub(r'[^\\x00-\\x7F]+','-', label.get_text())) * seconds_per_sample_mean,2)))\n",
    "    plt.xticks(locs[1:-1], labels_seconds[1:-1])\n",
    "\n",
    "    plt.savefig(\"./data/RQ/plots/averaged_total/\" + translate(baseline) + \".pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot averaged program mental work load and mean\n",
    "    style_it=itertools.cycle(STYLES)\n",
    "    color_it=itertools.cycle(COLORS)\n",
    "    plt.figure()\n",
    "    color=next(color_it)\n",
    "    linestyle=next(style_it)\n",
    "    plt.plot(program_per_baseline_averaged_minus_baseline, color=color, linestyle=linestyle, label=f\"$ProgCompr_{{{translate(baseline)}}}$ > {translate(baseline)}\")\n",
    "    if baseline_marker_75_pc > 0:\n",
    "        plt.axvline(x=baseline_marker_75_pc, linestyle='solid', color='k')\n",
    "        plt.text(baseline_marker_75_pc,0.01,'75% ', ha='right', va='bottom', transform=plt.gca().get_xaxis_transform())\n",
    "    plt.legend(handlelength=3)\n",
    "    plt.xlabel(f\"Seconds\")\n",
    "    plt.ylabel(\"Mental Load\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "    # Convert samples in seconds\n",
    "    locs, labels = plt.xticks()\n",
    "    labels_seconds = []\n",
    "    for label in labels:\n",
    "        labels_seconds.append(str(round(float(re.sub(r'[^\\x00-\\x7F]+','-', label.get_text())) * seconds_per_sample_mean,2)))\n",
    "    plt.xticks(locs[1:-1], labels_seconds[1:-1])\n",
    "\n",
    "    plt.savefig(\"./data/RQ/plots/averaged_program_comprehension_with_baseline/\" + translate(baseline) + \".pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot averaged program mental work load and mean with more detail\n",
    "    style_it=itertools.cycle(STYLES)\n",
    "    color_it=itertools.cycle(COLORS)\n",
    "    plt.figure()\n",
    "    color=next(color_it)\n",
    "    linestyle=next(style_it)\n",
    "    plt.plot(program_per_baseline_averaged, color=color, linestyle=linestyle, label=f\"$ProgCompr_{{{translate(baseline)}}}$\")\n",
    "    color=next(color_it)\n",
    "    linestyle=next(style_it)\n",
    "    plt.plot(program_per_baseline_averaged_minus_baseline, color=color, linestyle=linestyle, label=f\"$ProgCompr_{{{translate(baseline)}}}$ > {translate(baseline)}\")\n",
    "    plt.axhline(y=np.mean(baseline_averaged), color='r', label=f\"{translate(baseline)} Mean\")\n",
    "    if baseline_marker_75_pc > 0:\n",
    "        plt.axvline(x=baseline_marker_75_pc, linestyle='solid', color='k')\n",
    "        plt.text(baseline_marker_75_pc,0.01,'75% ', ha='right', va='bottom', transform=plt.gca().get_xaxis_transform())\n",
    "    plt.legend(handlelength=3)\n",
    "    plt.xlabel(f\"Seconds\")\n",
    "    plt.ylabel(\"Mental Load\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "    # Convert samples in seconds\n",
    "    locs, labels = plt.xticks()\n",
    "    labels_seconds = []\n",
    "    for label in labels:\n",
    "        labels_seconds.append(str(round(float(re.sub(r'[^\\x00-\\x7F]+','-', label.get_text())) * seconds_per_sample_mean,2)))\n",
    "    plt.xticks(locs[1:-1], labels_seconds[1:-1])\n",
    "\n",
    "    plt.savefig(\"./data/RQ/plots/averaged_program_comprehension_with_baseline/\" + translate(baseline) + \"_detail.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot averaged program mental work load and mean with more detail in subplot\n",
    "    style_it=itertools.cycle(STYLES)\n",
    "    color_it=itertools.cycle(COLORS)\n",
    "    figure = total_detail_axs[count]\n",
    "    color=next(color_it)\n",
    "    linestyle=next(style_it)\n",
    "    figure.plot(program_per_baseline_averaged, color=color, linestyle=linestyle, label=f\"$ProgCompr_{{{translate(baseline)}}}$\")\n",
    "    color=next(color_it)\n",
    "    linestyle=next(style_it)\n",
    "    figure.plot(program_per_baseline_averaged_minus_baseline, color=color, linestyle=linestyle, label=f\"$ProgCompr_{{{translate(baseline)}}}$ > {translate(baseline)}\")\n",
    "    figure.axhline(y=np.mean(baseline_averaged), color='r', label=f\"{translate(baseline)} Mean\")\n",
    "    if baseline_marker_75_pc > 0:\n",
    "        figure.axvline(x=baseline_marker_75_pc, linestyle='solid', color='k')\n",
    "        figure.text(baseline_marker_75_pc,0.01,'75% ', ha='right', va='bottom', transform=figure.get_xaxis_transform())\n",
    "    figure.set_title(translate(baseline))\n",
    "    figure.legend(handlelength=3, loc=2)\n",
    "    figure.set_xlabel(f\"Samples\")\n",
    "    figure.set_ylabel(\"Mental Load\")\n",
    "    figure.grid(False)\n",
    "    figure.spines['top'].set_visible(False)\n",
    "    figure.spines['right'].set_visible(False)\n",
    "    count += 1\n",
    "\n",
    "plt.figure(total_detail_fig)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./data/RQ/plots/averaged_program_comprehension_with_baseline/total_detail.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "# Average program comprehension mental work load\n",
    "for i in range(0, len(program_averaged)):\n",
    "    program_averaged[i] = program_averaged[i] / program_sample_count[i]\n",
    "df_averaged_over_all = pd.concat([df_averaged_over_all, pd.DataFrame(data={\"Type\": \"Program\", \"MWL\": [program_averaged], \"MWL_Mean\": np.mean(program_averaged)})],ignore_index=True)\n",
    "\n",
    "# Plot averaged program comprehension mental work load and mean\n",
    "style_it=itertools.cycle(STYLES)\n",
    "color_it=itertools.cycle(COLORS)\n",
    "plt.figure()\n",
    "color=next(color_it)\n",
    "linestyle=next(style_it)\n",
    "p = plt.plot(program_averaged, color=color, linestyle=linestyle, label=\"ProgCompr\")\n",
    "plt.axhline(y=np.mean(program_averaged), color=color, linestyle=linestyle, label=\"ProgCompr Mean\")\n",
    "if program_marker_75 > 0:\n",
    "        plt.axvline(x=program_marker_75, linestyle='solid', color='k')\n",
    "        plt.text(program_marker_75,0.01,'75% ', ha='right', va='bottom', transform=plt.gca().get_xaxis_transform())\n",
    "plt.legend(handlelength=3)\n",
    "plt.xlabel(f\"Seconds\")\n",
    "plt.ylabel(\"Mental Load\")\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Convert samples in seconds\n",
    "locs, labels = plt.xticks()\n",
    "labels_seconds = []\n",
    "for label in labels:\n",
    "    labels_seconds.append(str(round(float(re.sub(r'[^\\x00-\\x7F]+','-', label.get_text())) * seconds_per_sample_mean,2)))\n",
    "plt.xticks(locs[1:-1], labels_seconds[1:-1])\n",
    "\n",
    "plt.savefig(\"./data/RQ/plots/averaged_total/program_comprehension.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "# Plot averaged program mental work load and mean total\n",
    "if len(total_plot) > 0:\n",
    "    style_it=itertools.cycle(STYLES)\n",
    "    color_it=itertools.cycle(COLORS)\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.vlines(x=200,ymin=0.6,ymax=1.7, color='white')\n",
    "\n",
    "for data in total_plot:\n",
    "    color=next(color_it)\n",
    "    linestyle=next(style_it)\n",
    "    plt.plot(data[0], color=color, linestyle=linestyle, label=data[1])\n",
    "\n",
    "if len(total_plot) > 0:\n",
    "    plt.legend(handlelength=3)\n",
    "    plt.xlabel(f\"Seconds\")\n",
    "    plt.ylabel(\"Mental Load\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "    # Convert samples in seconds\n",
    "    locs, labels = plt.xticks()\n",
    "    labels_seconds = []\n",
    "    for label in labels:\n",
    "        labels_seconds.append(str(round(float(re.sub(r'[^\\x00-\\x7F]+','-', label.get_text())) * seconds_per_sample_mean,2)))\n",
    "    plt.xticks(locs[1:-1], labels_seconds[1:-1])\n",
    "\n",
    "    plt.savefig(\"./data/RQ/plots/averaged_program_comprehension_with_baseline/total.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "# Plot averaged program mental work load and mean total\n",
    "if len(total_plot) > 0:\n",
    "    style_it=itertools.cycle(STYLES)\n",
    "    color_it=itertools.cycle(COLORS)\n",
    "    plt.figure()\n",
    "\n",
    "for data in total_program_split_by_baseline_plot:\n",
    "    color=next(color_it)\n",
    "    linestyle=next(style_it)\n",
    "    plt.plot(data[0], color=color, linestyle=linestyle, label=data[1])\n",
    "\n",
    "if len(total_plot) > 0:\n",
    "    plt.legend(handlelength=3)\n",
    "    plt.xlabel(f\"Seconds\")\n",
    "    plt.ylabel(\"Mental Load\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "    # Convert samples in seconds\n",
    "    locs, labels = plt.xticks()\n",
    "    labels_seconds = []\n",
    "    for label in labels:\n",
    "        labels_seconds.append(str(round(float(re.sub(r'[^\\x00-\\x7F]+','-', label.get_text())) * seconds_per_sample_mean,2)))\n",
    "    plt.xticks(locs[1:-1], labels_seconds[1:-1])\n",
    "\n",
    "    plt.savefig(\"./data/RQ/plots/averaged_program_comprehension_with_baseline/total_program_split_by_baseline.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "# Plot averaged program mental work load and mean total\n",
    "if len(total_baseline_plot) > 0:\n",
    "    style_it=itertools.cycle(STYLES)\n",
    "    color_it=itertools.cycle(COLORS)\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.vlines(x=200,ymin=1.3,ymax=3, color='white')\n",
    "\n",
    "for data in total_baseline_plot:\n",
    "    color=next(color_it)\n",
    "    linestyle=next(style_it)\n",
    "    p = plt.plot(data[0], color=color, linestyle=linestyle, label=data[1])\n",
    "    plt.axhline(y=np.mean(data[0]), color=color, linestyle=linestyle, label=f\"_{data[1]} Mean\", xmin=0.8)\n",
    "\n",
    "if len(total_baseline_plot) > 0:\n",
    "    color=next(color_it)\n",
    "    linestyle=next(style_it)\n",
    "    plt.plot(program_averaged, color=color, linestyle=linestyle, label=\"ProgCompr\")\n",
    "    plt.axhline(y=np.mean(program_averaged), color=color, linestyle=linestyle, label=\"_ProgCompr Mean\", xmin=0.8)\n",
    "    plt.text(0.9, 0.4, 'Mean', horizontalalignment='center', verticalalignment='center',transform=plt.gca().transAxes)\n",
    "    plt.legend(handlelength=3, loc='upper left')\n",
    "    plt.xlabel(f\"Seconds\")\n",
    "    plt.ylabel(\"Mental Load\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "    # Convert samples in seconds\n",
    "    locs, labels = plt.xticks()\n",
    "    labels_seconds = []\n",
    "    for label in labels:\n",
    "        labels_seconds.append(str(round(float(re.sub(r'[^\\x00-\\x7F]+','-', label.get_text())) * seconds_per_sample_mean,2)))\n",
    "    plt.xticks(locs[1:-1], labels_seconds[1:-1])\n",
    "\n",
    "    plt.savefig(\"./data/RQ/plots/averaged_total/total.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "df_averaged_over_all.to_csv(\"./data/RQ/averaged_over_all.csv\", sep=\";\", index=False)\n",
    "df_averaged_program_comprehension_with_baseline.to_csv(\"./data/RQ/averaged_program_comprehension_with_baseline.csv\", sep=\";\", index=False)\n",
    "df_averaged_program_comprehension_with_baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Statistic tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Calculate data for statistic tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mwl_raw = {}\n",
    "mwl_per_baseline = {}\n",
    "mwl_minus_baseline_per_baseline = {}\n",
    "\n",
    "mwl_program_total = []\n",
    "\n",
    "# Iterate over all baselines\n",
    "baselines = df_filtered[\"Baseline\"].unique()\n",
    "for baseline in tqdm(baselines):\n",
    "    # Init vars for baseline calculation\n",
    "    df_baseline = df_filtered[df_filtered[\"Baseline\"] == baseline]\n",
    "\n",
    "    raw = []\n",
    "    mwl = []\n",
    "    mwl_minus_baseline = []\n",
    "\n",
    "    for index, row in df_baseline.iterrows():\n",
    "        mwl_program_minus_baseline = row[\"MentalWorkLoad_Program_Minus_Baseline\"]\n",
    "        mwl_program = row[\"MentalWorkLoad_Program\"]\n",
    "        mwl_baseline = row[\"MentalWorkLoad_Baseline\"]\n",
    "\n",
    "        raw.append(np.mean(mwl_baseline))\n",
    "        mwl.append(np.mean(mwl_program))\n",
    "        mwl_program_total.append(np.mean(mwl_program))\n",
    "        mwl_minus_baseline.append(np.mean(mwl_program_minus_baseline))\n",
    "\n",
    "    mwl_raw[baseline] = raw\n",
    "    mwl_per_baseline[baseline] = mwl\n",
    "    mwl_minus_baseline_per_baseline[baseline] = mwl_minus_baseline\n",
    "\n",
    "mwl_raw[\"ProgCompr\"] = mwl_program_total"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Shapiro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/statistics\"):\n",
    "    os.makedirs(\"./data/RQ/statistics\")\n",
    "\n",
    "# Create dataframe\n",
    "df_shapiro = pd.DataFrame(columns = [\"Baseline\"])\n",
    "df_shapiro_mb = pd.DataFrame(columns = [\"Baseline\"])\n",
    "df_shapiro_raw = pd.DataFrame(columns = [\"Data\"])\n",
    "\n",
    "# Iterate over all baselines\n",
    "baselines = df_filtered[\"Baseline\"].unique()\n",
    "for baseline in baselines:\n",
    "    shapiro = stats.shapiro(mwl_per_baseline[baseline])\n",
    "    shapiro_mb = stats.shapiro(mwl_minus_baseline_per_baseline[baseline])\n",
    "    shapiro_raw = stats.shapiro(mwl_raw[baseline])\n",
    "    df_shapiro = pd.concat([df_shapiro, pd.DataFrame(data=[{\"Baseline\": translate(baseline), \"Shapiro_Statistic\": shapiro[0], \"Shapiro_pValue\": shapiro[1]}])],ignore_index=True)\n",
    "    df_shapiro_mb = pd.concat([df_shapiro_mb, pd.DataFrame(data=[{\"Baseline\": translate(baseline), \"Shapiro_Statistic\": shapiro_mb[0], \"Shapiro_pValue\": shapiro_mb[1]}])],ignore_index=True)\n",
    "    df_shapiro_raw = pd.concat([df_shapiro_raw, pd.DataFrame(data=[{\"Data\": translate(baseline), \"Shapiro_Statistic\": shapiro_raw[0], \"Shapiro_pValue\": shapiro_raw[1]}])],ignore_index=True)\n",
    "\n",
    "shapiro_raw = stats.shapiro(mwl_raw[\"ProgCompr\"])\n",
    "df_shapiro_raw = pd.concat([df_shapiro_raw, pd.DataFrame(data=[{\"Data\": \"ProgCompr\", \"Shapiro_Statistic\": shapiro_raw[0], \"Shapiro_pValue\": shapiro_raw[1]}])],ignore_index=True)\n",
    "\n",
    "# Save dataframe to file\n",
    "df_shapiro.to_csv(\"./data/RQ/statistics/shapiro_no_baseline_correction.csv\", sep=\";\", index=False)\n",
    "df_shapiro_mb.to_csv(\"./data/RQ/statistics/shapiro_with_baseline_correction.csv\", sep=\";\", index=False)\n",
    "df_shapiro_raw.to_csv(\"./data/RQ/statistics/shapiro_raw.csv\", sep=\";\", index=False)\n",
    "pd.set_option(\"display.precision\", 20)\n",
    "df_shapiro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 20)\n",
    "df_shapiro_mb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 20)\n",
    "df_shapiro_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Mann Whitney U & Kruskal Wallis test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/statistics\"):\n",
    "    os.makedirs(\"./data/RQ/statistics\")\n",
    "\n",
    "# Create dataframe\n",
    "df_significance_averaged_time = pd.DataFrame(columns = [\"Baseline1\", \"Baseline2\",\"Kruskal_Statistic\",\"Kruskal_pValue\", \"MWU_Statistic\", \"MWU_pValue\"])\n",
    "df_significance_averaged_time_mb = pd.DataFrame(columns = [\"Baseline1\", \"Baseline2\",\"Kruskal_Statistic\",\"Kruskal_pValue\", \"MWU_Statistic\", \"MWU_pValue\"])\n",
    "df_significance_averaged_time_raw = pd.DataFrame(columns = [\"1\", \"2\",\"Kruskal_Statistic\",\"Kruskal_pValue\", \"MWU_Statistic\", \"MWU_pValue\"])\n",
    "\n",
    "# Iterate over all combinations of baselines\n",
    "for combination in list(itertools.combinations(mwl_per_baseline.keys(), 2)):\n",
    "    # Load mental work load data for first baseline of combination\n",
    "    group1 = mwl_per_baseline[combination[0]]\n",
    "    group1_mb = mwl_minus_baseline_per_baseline[combination[0]]\n",
    "    group1_raw = mwl_raw[combination[0]]\n",
    "\n",
    "    # Load mental work load data for second baseline of combination\n",
    "    group2 = mwl_per_baseline[combination[1]]\n",
    "    group2_mb = mwl_minus_baseline_per_baseline[combination[1]]\n",
    "    group2_raw = mwl_raw[combination[1]]\n",
    "\n",
    "    # Perform Mann Whitney U test\n",
    "    mwu = stats.mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "    mwu_mb = stats.mannwhitneyu(group1_mb, group2_mb, alternative='two-sided')\n",
    "    mwu_raw = stats.mannwhitneyu(group1_raw, group2_raw, alternative='two-sided')\n",
    "\n",
    "    # Add to dataframe\n",
    "    df_significance_averaged_time = pd.concat([df_significance_averaged_time, pd.DataFrame(data=[{\"Baseline1\": translate(combination[0]), \"Baseline2\": translate(combination[1]), \"MWU_Statistic\": mwu[0], \"MWU_pValue\": mwu[1]}])],ignore_index=True)\n",
    "    df_significance_averaged_time_mb = pd.concat([df_significance_averaged_time_mb, pd.DataFrame(data=[{\"Baseline1\": translate(combination[0]), \"Baseline2\": translate(combination[1]), \"MWU_Statistic\": mwu_mb[0], \"MWU_pValue\": mwu_mb[1]}])],ignore_index=True)\n",
    "    df_significance_averaged_time_raw = pd.concat([df_significance_averaged_time_raw, pd.DataFrame(data=[{\"1\": translate(combination[0]), \"2\": translate(combination[1]), \"MWU_Statistic\": mwu_raw[0], \"MWU_pValue\": mwu_raw[1]}])],ignore_index=True)\n",
    "\n",
    "baselines = df_filtered[\"Baseline\"].unique()\n",
    "for baseline in baselines:\n",
    "    group1_raw = mwl_raw[\"ProgCompr\"]\n",
    "    group2_raw = mwl_raw[baseline]\n",
    "\n",
    "    # Perform Mann Whitney U test\n",
    "    mwu_raw = stats.mannwhitneyu(group1_raw, group2_raw, alternative='two-sided')\n",
    "\n",
    "    # Add to dataframe\n",
    "    df_significance_averaged_time_raw = pd.concat([df_significance_averaged_time_raw, pd.DataFrame(data=[{\"1\": \"ProgCompr\", \"2\": translate(baseline), \"MWU_Statistic\": mwu_raw[0], \"MWU_pValue\": mwu_raw[1]}])],ignore_index=True)\n",
    "\n",
    "# Iterate over all baselines\n",
    "baselines = df_filtered[\"Baseline\"].unique()\n",
    "kruskal_data = []\n",
    "kruskal_data_mb = []\n",
    "kruskal_data_raw = []\n",
    "for baseline in baselines:\n",
    "    kruskal_data.append(mwl_per_baseline[baseline])\n",
    "    kruskal_data_mb.append(mwl_minus_baseline_per_baseline[baseline])\n",
    "    kruskal_data_raw.append(mwl_raw[baseline])\n",
    "\n",
    "kruskal_data_raw.append(mwl_raw[\"ProgCompr\"])\n",
    "\n",
    "# Perform kruskal wallis test\n",
    "kruskal = stats.kruskal(*kruskal_data)\n",
    "kruskal_mb = stats.kruskal(*kruskal_data_mb)\n",
    "kruskal_raw = stats.kruskal(*kruskal_data_raw)\n",
    "# Add to dataframe\n",
    "df_significance_averaged_time = pd.concat([df_significance_averaged_time, pd.DataFrame(data=[{\"Kruskal_Statistic\": kruskal[0], \"Kruskal_pValue\": kruskal[1]}])],ignore_index=True)\n",
    "df_significance_averaged_time_mb = pd.concat([df_significance_averaged_time_mb, pd.DataFrame(data=[{\"Kruskal_Statistic\": kruskal_mb[0], \"Kruskal_pValue\": kruskal_mb[1]}])],ignore_index=True)\n",
    "df_significance_averaged_time_raw = pd.concat([df_significance_averaged_time_raw, pd.DataFrame(data=[{\"Kruskal_Statistic\": kruskal_raw[0], \"Kruskal_pValue\": kruskal_raw[1]}])],ignore_index=True)\n",
    "\n",
    "# Save dataframe to file\n",
    "df_significance_averaged_time.to_csv(\"./data/RQ/statistics/significance_no_baseline_correction.csv\", sep=\";\", index=False)\n",
    "df_significance_averaged_time_mb.to_csv(\"./data/RQ/statistics/significance_with_baseline_correction.csv\", sep=\";\", index=False)\n",
    "df_significance_averaged_time_raw.to_csv(\"./data/RQ/statistics/significance_raw.csv\", sep=\";\", index=False)\n",
    "pd.set_option(\"display.precision\", 20)\n",
    "df_significance_averaged_time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 20)\n",
    "df_significance_averaged_time_mb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 20)\n",
    "df_significance_averaged_time_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cliff's Delta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/statistics\"):\n",
    "    os.makedirs(\"./data/RQ/statistics\")\n",
    "\n",
    "# Create dataframe\n",
    "df_cliffs = pd.DataFrame(columns = [\"Baseline1\", \"Baseline2\",\"D\",\"Res\"])\n",
    "df_cliffs_mb = pd.DataFrame(columns = [\"Baseline1\", \"Baseline2\",\"D\",\"Res\"])\n",
    "df_cliffs_raw = pd.DataFrame(columns = [\"1\", \"2\",\"D\",\"Res\"])\n",
    "\n",
    "# Iterate over all combinations of baselines\n",
    "for combination in list(itertools.combinations(mwl_per_baseline.keys(), 2)):\n",
    "    # Load mental work load data for first baseline of combination\n",
    "    group1 = mwl_per_baseline[combination[0]]\n",
    "    group1_mb = mwl_minus_baseline_per_baseline[combination[0]]\n",
    "    group1_raw = mwl_raw[combination[0]]\n",
    "\n",
    "    # Load mental work load data for second baseline of combination\n",
    "    group2 = mwl_per_baseline[combination[1]]\n",
    "    group2_mb = mwl_minus_baseline_per_baseline[combination[1]]\n",
    "    group2_raw = mwl_raw[combination[1]]\n",
    "\n",
    "    # Perform Cliffs Delta\n",
    "    d, res = cliffs_delta(group1, group2)\n",
    "    d_mb, res_mb = cliffs_delta(group1_mb, group2_mb)\n",
    "    d_raw, res_raw = cliffs_delta(group1_raw, group2_raw)\n",
    "\n",
    "    # Add to dataframe\n",
    "    df_cliffs = pd.concat([df_cliffs, pd.DataFrame(data=[{\"Baseline1\": translate(combination[0]), \"Baseline2\": translate(combination[1]), \"D\": d, \"Res\": res}])],ignore_index=True)\n",
    "\n",
    "    # Add to dataframe\n",
    "    df_cliffs_mb = pd.concat([df_cliffs_mb, pd.DataFrame(data=[{\"Baseline1\": translate(combination[0]), \"Baseline2\": translate(combination[1]), \"D\": d_mb, \"Res\": res_mb}])],ignore_index=True)\n",
    "\n",
    "    # Add to dataframe\n",
    "    df_cliffs_raw = pd.concat([df_cliffs_raw, pd.DataFrame(data=[{\"1\": translate(combination[0]), \"2\": translate(combination[1]), \"D\": d_raw, \"Res\": res_raw}])],ignore_index=True)\n",
    "\n",
    "baselines = df_filtered[\"Baseline\"].unique()\n",
    "for baseline in baselines:\n",
    "    group1_raw = mwl_raw[\"ProgCompr\"]\n",
    "    group2_raw = mwl_raw[baseline]\n",
    "\n",
    "    # Perform Mann Whitney U test\n",
    "    d_raw, res_raw = cliffs_delta(group1_raw, group2_raw)\n",
    "\n",
    "    # Add to dataframe\n",
    "    df_cliffs_raw = pd.concat([df_cliffs_raw, pd.DataFrame(data=[{\"1\": \"ProgCompr\", \"2\": translate(baseline), \"D\": d_raw, \"Res\": res_raw}])],ignore_index=True)\n",
    "\n",
    "# Save dataframe to file\n",
    "df_cliffs.to_csv(\"./data/RQ/statistics/cliffs_delta_no_baseline_correction.csv\", sep=\";\", index=False)\n",
    "df_cliffs_mb.to_csv(\"./data/RQ/statistics/cliffs_delta_with_baseline_correction.csv\", sep=\";\", index=False)\n",
    "df_cliffs_raw.to_csv(\"./data/RQ/statistics/cliffs_delta_raw.csv\", sep=\";\", index=False)\n",
    "pd.set_option(\"display.precision\", 20)\n",
    "df_cliffs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 20)\n",
    "df_cliffs_mb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 20)\n",
    "df_cliffs_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Eye-tracking"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Fixation Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read in the fixation data\n",
    "df_fixation = pd.read_csv('./data/filteredData/fixation_stats.csv', sep=\";\")\n",
    "\n",
    "# Transform fixation strings to lists\n",
    "df_fixation[\"Fixation_startT\"] = df_fixation[\"Fixation_startT\"].apply(string_to_list_string)\n",
    "df_fixation[\"Fixation_endT\"] = df_fixation[\"Fixation_endT\"].apply(string_to_list_string)\n",
    "df_fixation[\"Fixation_x\"] = df_fixation[\"Fixation_x\"].apply(string_to_list_string)\n",
    "df_fixation[\"Fixation_y\"] = df_fixation[\"Fixation_y\"].apply(string_to_list_string)\n",
    "df_fixation[\"Fixation_x_range\"] = df_fixation[\"Fixation_x_range\"].apply(string_to_list_string)\n",
    "df_fixation[\"Fixation_y_range\"] = df_fixation[\"Fixation_y_range\"].apply(string_to_list_string)\n",
    "df_fixation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate and plot scanpath for every task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/plots/scanpaths\"):\n",
    "    os.makedirs(\"./data/RQ/plots/scanpaths\")\n",
    "\n",
    "print(\"Generate scanpath for snippets\")\n",
    "snippets = df_fixation[\"Algorithm\"].unique()\n",
    "for snippet in tqdm(snippets):\n",
    "    # Load snippet image\n",
    "    image = Image.open(f\"./data/Snippets/{snippet}.png\")\n",
    "\n",
    "    # Set snippet boundaries\n",
    "    width, height = image.size\n",
    "    x_low = int(1920 * 0.5) - int(width / 2)\n",
    "    y_low = int(1080 * 0.5) - int(height / 2)\n",
    "    x_high = int(1920 * 0.5) + int(width / 2)\n",
    "    y_high = int(1080 * 0.5) + int(height / 2)\n",
    "\n",
    "    # Get fixation data for snippet\n",
    "    df_algo = df_fixation[(df_fixation[\"Algorithm\"] == snippet) & (df_fixation[\"Type\"] == \"PC\")]\n",
    "    participants = df_algo[\"Participant\"].unique()\n",
    "\n",
    "    # Iterate over all participants\n",
    "    for participant in participants:\n",
    "        if len(df_algo[df_algo[\"Participant\"] == participant]) != 1:\n",
    "            raise ValueError(\"Participant has more than exactly one entry\")\n",
    "\n",
    "        # Get fixation data for participant\n",
    "        df_algo_part = df_algo[df_algo[\"Participant\"] == participant].reset_index().iloc[0]\n",
    "        current_image = image.copy()\n",
    "        fixation_start_array = np.array(eval(df_algo_part[\"Fixation_startT\"]))\n",
    "        fixation_end_array = np.array(eval(df_algo_part[\"Fixation_endT\"]))\n",
    "        fixation_x_coordinates = np.array(eval(df_algo_part[\"Fixation_x\"]))\n",
    "        fixation_y_coordinates = np.array(eval(df_algo_part[\"Fixation_y\"]))\n",
    "        fixations = np.stack((fixation_start_array, fixation_end_array, fixation_x_coordinates, fixation_y_coordinates), axis=1)\n",
    "\n",
    "        # Set plot settings\n",
    "        cm = plt.cm.get_cmap('inferno')\n",
    "        cm = plt.cm.ScalarMappable(cmap=cm, norm=plt.Normalize(vmin=0, vmax=len(fixations)))\n",
    "        patches = []\n",
    "\n",
    "        # Calculate fixations\n",
    "        for idx, (start, end, x, y) in enumerate(fixations):\n",
    "            if x_low <= x <= x_high and y_low <= y <= y_high:\n",
    "                x = int(x)\n",
    "                y = int(y)\n",
    "                x = x - x_low\n",
    "                y = y - y_low\n",
    "                color = cm.to_rgba(idx)\n",
    "                patches.append(Circle((x, y), radius=10, color=color, alpha=0.5))\n",
    "\n",
    "        # Plot fixations\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(current_image)\n",
    "        for p in patches:\n",
    "            ax.add_patch(p)\n",
    "\n",
    "        # Plot paths\n",
    "        for idx in range(len(fixations) - 1):\n",
    "            start_x = fixations[idx, 2] - x_low\n",
    "            start_y = fixations[idx, 3] - y_low\n",
    "            end_x = fixations[idx+1, 2] - x_low\n",
    "            end_y = fixations[idx+1, 3] - y_low\n",
    "            if 0 <= start_x <= width and 0 <= start_y <= height and 0 <= end_x <= width and 0 <= end_y <= height:\n",
    "                color = cm.to_rgba(idx)\n",
    "                ax.plot([start_x, end_x], [start_y, end_y], color=color, alpha=0.3)\n",
    "\n",
    "        # Plot settings\n",
    "        plt.xlim(0, width)\n",
    "        plt.ylim(height, 0)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Create folder if not exists\n",
    "        if not os.path.exists(f\"./data/RQ/plots/scanpaths/snippets/{snippet}\"):\n",
    "            os.makedirs(f\"./data/RQ/plots/scanpaths/snippets/{snippet}\")\n",
    "\n",
    "        # Save plot\n",
    "        plt.savefig(f\"./data/RQ/plots/scanpaths/snippets/{snippet}/{participant}.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "print(\"Generate scanpath for baselines\")\n",
    "baseline_tasks = df_filtered[\"BaselineTask\"].unique()\n",
    "for baseline_task in tqdm(baseline_tasks):\n",
    "    # Load snippet image\n",
    "    image = Image.open(f\"./data/BaselineSnippets/{baseline_task}.png\")\n",
    "\n",
    "    # Set snippet boundaries\n",
    "    width, height = image.size\n",
    "    x_low = int(1920 * 0.5) - int(width / 2)\n",
    "    y_low = int(1080 * 0.5) - int(height / 2)\n",
    "    x_high = int(1920 * 0.5) + int(width / 2)\n",
    "    y_high = int(1080 * 0.5) + int(height / 2)\n",
    "\n",
    "    # Get fixation data for snippet\n",
    "    df_baseline = df_filtered[df_filtered[\"BaselineTask\"] == baseline_task]\n",
    "    participants = df_baseline[\"Participant\"].unique()\n",
    "\n",
    "    # Iterate over all participants\n",
    "    for participant in participants:\n",
    "        df_temp = df_filtered[(df_filtered[\"Participant\"] == participant) & (df_filtered[\"BaselineTask\"] == baseline_task)][\"Algorithm\"]\n",
    "        for i in range(0, len(df_temp)):\n",
    "            algorithm = df_temp.iloc[i]\n",
    "            df_algo = df_fixation[(df_fixation[\"Algorithm\"] == algorithm) & (df_fixation[\"Type\"] == \"Baseline\")]\n",
    "            if len(df_algo[df_algo[\"Participant\"] == participant]) != 1:\n",
    "                raise ValueError(\"Participant has more than exactly one entry\")\n",
    "\n",
    "            # Get fixation data for participant\n",
    "            df_algo_part = df_algo[df_algo[\"Participant\"] == participant].reset_index().iloc[0]\n",
    "            current_image = image.copy()\n",
    "            fixation_start_array = np.array(eval(df_algo_part[\"Fixation_startT\"]))\n",
    "            fixation_end_array = np.array(eval(df_algo_part[\"Fixation_endT\"]))\n",
    "            fixation_x_coordinates = np.array(eval(df_algo_part[\"Fixation_x\"]))\n",
    "            fixation_y_coordinates = np.array(eval(df_algo_part[\"Fixation_y\"]))\n",
    "            fixations = np.stack((fixation_start_array, fixation_end_array, fixation_x_coordinates, fixation_y_coordinates), axis=1)\n",
    "\n",
    "            # Set plot settings\n",
    "            cm = plt.cm.get_cmap('inferno')\n",
    "            cm = plt.cm.ScalarMappable(cmap=cm, norm=plt.Normalize(vmin=0, vmax=len(fixations)))\n",
    "            patches = []\n",
    "\n",
    "            # Calculate fixations\n",
    "            for idx, (start, end, x, y) in enumerate(fixations):\n",
    "                if x_low <= x <= x_high and y_low <= y <= y_high:\n",
    "                    x = int(x)\n",
    "                    y = int(y)\n",
    "                    x = x - x_low\n",
    "                    y = y - y_low\n",
    "                    color = cm.to_rgba(idx)\n",
    "                    patches.append(Circle((x, y), radius=10, color=color, alpha=0.5))\n",
    "\n",
    "            # Plot fixations\n",
    "            fig, ax = plt.subplots(1)\n",
    "            ax.imshow(current_image)\n",
    "            for p in patches:\n",
    "                ax.add_patch(p)\n",
    "\n",
    "            # Plot paths\n",
    "            for idx in range(len(fixations) - 1):\n",
    "                start_x = fixations[idx, 2] - x_low\n",
    "                start_y = fixations[idx, 3] - y_low\n",
    "                end_x = fixations[idx+1, 2] - x_low\n",
    "                end_y = fixations[idx+1, 3] - y_low\n",
    "                if 0 <= start_x <= width and 0 <= start_y <= height and 0 <= end_x <= width and 0 <= end_y <= height:\n",
    "                    color = cm.to_rgba(idx)\n",
    "                    ax.plot([start_x, end_x], [start_y, end_y], color=color, alpha=0.3)\n",
    "\n",
    "            # Plot settings\n",
    "            plt.xlim(0, width)\n",
    "            plt.ylim(height, 0)\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Create folder if not exists\n",
    "            if not os.path.exists(f\"./data/RQ/plots/scanpaths/baseline_tasks/{baseline_task}\"):\n",
    "                os.makedirs(f\"./data/RQ/plots/scanpaths/baseline_tasks/{baseline_task}\")\n",
    "\n",
    "            # Save plot\n",
    "            if baseline_task == \"Rest_baseline\":\n",
    "                plt.savefig(f\"./data/RQ/plots/scanpaths/baseline_tasks/{baseline_task}/{participant}_{i}.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "            else:\n",
    "                plt.savefig(f\"./data/RQ/plots/scanpaths/baseline_tasks/{baseline_task}/{participant}.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Heatmaps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/plots/heatmaps\"):\n",
    "    os.makedirs(\"./data/RQ/plots/heatmaps\")\n",
    "if not os.path.exists(\"./data/RQ/plots/heatmaps/snippets\"):\n",
    "    os.makedirs(\"./data/RQ/plots/heatmaps/snippets\")\n",
    "if not os.path.exists(\"./data/RQ/plots/heatmaps/baseline_tasks\"):\n",
    "    os.makedirs(\"./data/RQ/plots/heatmaps/baseline_tasks\")\n",
    "\n",
    "print(\"Generate heatmaps for snippets\")\n",
    "# Iterate over all snippets\n",
    "snippets = df_fixation[\"Algorithm\"].unique()\n",
    "for snippet in tqdm(snippets):\n",
    "    # Load snippet image\n",
    "    image = Image.open(f\"./data/Snippets/{snippet}.png\")\n",
    "\n",
    "    # Set snippet boundaries\n",
    "    width, height = image.size\n",
    "    x_low = int(1920 * 0.5) - int(width / 2)\n",
    "    y_low = int(1080 * 0.5) - int(height / 2)\n",
    "    x_high = int(1920 * 0.5) + int(width / 2)\n",
    "    y_high = int(1080 * 0.5) + int(height / 2)\n",
    "\n",
    "    # Get fixation data for snippet\n",
    "    df_algo = df_fixation[(df_fixation[\"Algorithm\"] == snippet) & (df_fixation[\"Type\"] == \"PC\")]\n",
    "    participants = df_algo[\"Participant\"].unique()\n",
    "\n",
    "    heatmap_width = math.floor(1920/20)\n",
    "    heatmap_height = math.floor(1080/20)\n",
    "    snippet_data = [ [0]* heatmap_height for i in range(heatmap_width)]\n",
    "    data_max = 0\n",
    "\n",
    "    # Fill array for heatmap\n",
    "    for index, row in df_algo.iterrows():\n",
    "        fixation_start_array = np.array(eval(row[\"Fixation_startT\"]))\n",
    "        fixation_end_array = np.array(eval(row[\"Fixation_endT\"]))\n",
    "        fixation_x_coordinates = np.array(eval(row[\"Fixation_x\"]))\n",
    "        fixation_y_coordinates = np.array(eval(row[\"Fixation_y\"]))\n",
    "        fixations = np.stack((fixation_start_array, fixation_end_array, fixation_x_coordinates, fixation_y_coordinates), axis=1)\n",
    "\n",
    "        for idx, (start, end, x, y) in enumerate(fixations):\n",
    "            if x_low <= x <= x_high and y_low <= y <= y_high:\n",
    "                x = int(x)\n",
    "                y = int(y)\n",
    "                x = x - x_low\n",
    "                y = y - y_low\n",
    "                snippet_data[math.floor(x/20)][math.floor(y/20)] += 1\n",
    "                data_max = max(data_max, snippet_data[math.floor(x/20)][math.floor(y/20)])\n",
    "\n",
    "    # Set plot settings\n",
    "    cm = plt.cm.get_cmap('jet')\n",
    "    cm = plt.cm.ScalarMappable(cmap=cm, norm=plt.Normalize(vmin=1, vmax=data_max))\n",
    "    patches = []\n",
    "\n",
    "    # Calculate fixations\n",
    "    for x in range(0, heatmap_width):\n",
    "        for y in range(0, heatmap_height):\n",
    "            data = snippet_data[x][y]\n",
    "            if data > 0:\n",
    "                color = cm.to_rgba(data)\n",
    "                patches.append(Rectangle((x*20, y*20), 19, 19, color=color, alpha=0.3))\n",
    "\n",
    "    # Plot fixations\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    for p in patches:\n",
    "        ax.add_patch(p)\n",
    "\n",
    "    # Plot settings\n",
    "    plt.xlim(0, width)\n",
    "    plt.ylim(height, 0)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    plt.savefig(f\"./data/RQ/plots/heatmaps/snippets/{snippet}.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "print(\"Generate heatmaps for baseline tasks\")\n",
    "# Iterate over all baseline tasks\n",
    "baseline_tasks = df_filtered[\"BaselineTask\"].unique()\n",
    "for baseline_task in tqdm(baseline_tasks):\n",
    "    # Load snippet image\n",
    "    image = Image.open(f\"./data/BaselineSnippets/{baseline_task}.png\")\n",
    "\n",
    "    # Set snippet boundaries\n",
    "    width, height = image.size\n",
    "    x_low = int(1920 * 0.5) - int(width / 2)\n",
    "    y_low = int(1080 * 0.5) - int(height / 2)\n",
    "    x_high = int(1920 * 0.5) + int(width / 2)\n",
    "    y_high = int(1080 * 0.5) + int(height / 2)\n",
    "\n",
    "    heatmap_width = math.floor(1920/20)\n",
    "    heatmap_height = math.floor(1080/20)\n",
    "    snippet_data = [ [0]* heatmap_height for i in range(heatmap_width)]\n",
    "    data_max = 0\n",
    "\n",
    "    # Get fixation data for snippet\n",
    "    df_baseline_task = df_filtered[(df_filtered[\"BaselineTask\"] == baseline_task)]\n",
    "\n",
    "\n",
    "    for index, row in df_baseline_task.iterrows():\n",
    "        df_algo = df_fixation[(df_fixation[\"Algorithm\"] == row[\"Algorithm\"]) & (df_fixation[\"Participant\"] == row[\"Participant\"]) & (df_fixation[\"Type\"] == \"Baseline\")]\n",
    "\n",
    "        # Fill array for heatmap\n",
    "        for index, row in df_algo.iterrows():\n",
    "            fixation_start_array = np.array(eval(row[\"Fixation_startT\"]))\n",
    "            fixation_end_array = np.array(eval(row[\"Fixation_endT\"]))\n",
    "            fixation_x_coordinates = np.array(eval(row[\"Fixation_x\"]))\n",
    "            fixation_y_coordinates = np.array(eval(row[\"Fixation_y\"]))\n",
    "            fixations = np.stack((fixation_start_array, fixation_end_array, fixation_x_coordinates, fixation_y_coordinates), axis=1)\n",
    "\n",
    "            for idx, (start, end, x, y) in enumerate(fixations):\n",
    "                if x_low <= x <= x_high and y_low <= y <= y_high:\n",
    "                    x = int(x)\n",
    "                    y = int(y)\n",
    "                    x = x - x_low\n",
    "                    y = y - y_low\n",
    "                    snippet_data[math.floor(x/20)][math.floor(y/20)] += 1\n",
    "                    data_max = max(data_max, snippet_data[math.floor(x/20)][math.floor(y/20)])\n",
    "\n",
    "    # Set plot settings\n",
    "    cm = plt.cm.get_cmap('jet')\n",
    "    cm = plt.cm.ScalarMappable(cmap=cm, norm=plt.Normalize(vmin=1, vmax=data_max))\n",
    "    patches = []\n",
    "\n",
    "    # Calculate fixations\n",
    "    for x in range(0, heatmap_width):\n",
    "        for y in range(0, heatmap_height):\n",
    "            data = snippet_data[x][y]\n",
    "            if data > 0:\n",
    "                color = cm.to_rgba(data)\n",
    "                patches.append(Rectangle((x*20, y*20), 19, 19, color=color, alpha=0.4))\n",
    "\n",
    "    # Plot fixations\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    for p in patches:\n",
    "        ax.add_patch(p)\n",
    "\n",
    "    # Plot settings\n",
    "    plt.xlim(0, width)\n",
    "    plt.ylim(height, 0)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    plt.savefig(f\"./data/RQ/plots/heatmaps/baseline_tasks/{baseline_task}.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Topoplots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_topo = pd.read_csv(\"./data/RQ/topo_data/data.csv\", sep=\";\")\n",
    "\n",
    "channel_len = 751\n",
    "\n",
    "columns = []\n",
    "for i in range(1,channel_len+1):\n",
    "    columns.append(str(i))\n",
    "\n",
    "columns_with_channel = [\"Channel\"]\n",
    "columns_with_channel.extend(columns)\n",
    "\n",
    "df_topo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generate eeg data for topoplots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/topo_data/raw/eeg\"):\n",
    "    os.makedirs(\"./data/RQ/topo_data/raw/eeg\")\n",
    "\n",
    "# Read in montage and calculate the electrode positions\n",
    "montage_path = \"./data/EEG/AC-64.bvef\"\n",
    "montage = mne.channels.read_custom_montage(montage_path, head_size=0.085)\n",
    "\n",
    "for index, row in tqdm(df_topo.iterrows(), total=df_topo.shape[0]):\n",
    "    participant = row[\"Participant\"]\n",
    "    snippet = row[\"Algorithm\"]\n",
    "\n",
    "    # Get the path for the baseline eeg data\n",
    "    baseline_eeg_path = row[\"BaselineEEG\"]\n",
    "\n",
    "    # Read in the baseline eeg data\n",
    "    eeg_data_baseline = mne.io.read_raw_fif(baseline_eeg_path, verbose='ERROR')\n",
    "\n",
    "    # Get raw channel data and do mean average referencing\n",
    "    eeg_data_baseline_raw = eeg_data_baseline.get_data()\n",
    "    eeg_data_baseline_ref = eeg_data_baseline_raw - np.mean(eeg_data_baseline_raw, axis=0)\n",
    "\n",
    "    # Set montage\n",
    "    eeg_data_baseline.set_montage(montage, verbose='ERROR')\n",
    "\n",
    "    # Save baseline eeg data for topo\n",
    "    eeg_data_baseline_topo = mne.io.RawArray(eeg_data_baseline_ref, eeg_data_baseline.info, verbose='ERROR')\n",
    "    eeg_data_baseline_topo.save(f\"./data/RQ/topo_data/raw/eeg/{participant}_{snippet}_baseline_raw.fif\", verbose='ERROR')\n",
    "\n",
    "    # Get the path for the program eeg data\n",
    "    program_eeg_path = row[\"ProgramEEG\"]\n",
    "\n",
    "    # Read in the program eeg data\n",
    "    eeg_data_program = mne.io.read_raw_fif(program_eeg_path, verbose='ERROR')\n",
    "\n",
    "    # Get raw channel data and do mean average referencing\n",
    "    eeg_data_program_raw = eeg_data_program.get_data()\n",
    "    eeg_data_program_ref = eeg_data_program_raw - np.mean(eeg_data_program_raw, axis=0)\n",
    "\n",
    "    # Set montage\n",
    "    eeg_data_program.set_montage(montage, verbose='ERROR')\n",
    "\n",
    "    # Save program eeg data for topo\n",
    "    eeg_data_progra_topo = mne.io.RawArray(eeg_data_program_ref, eeg_data_program.info, verbose='ERROR')\n",
    "    eeg_data_progra_topo.save(f\"./data/RQ/topo_data/raw/eeg/{participant}_{snippet}_program_raw.fif\", verbose='ERROR')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Calculate PSDS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/topo_data/raw/psds\"):\n",
    "    os.makedirs(\"./data/RQ/topo_data/raw/psds\")\n",
    "\n",
    "# Read montage data\n",
    "montage_path = \"./data/EEG/AC-64.bvef\"\n",
    "montage = mne.channels.read_custom_montage(montage_path, head_size=0.085)\n",
    "\n",
    "for index, row in tqdm(df_topo.iterrows(), total=df_topo.shape[0]):\n",
    "    participant = row[\"Participant\"]\n",
    "    snippet = row[\"Algorithm\"]\n",
    "\n",
    "    # Read baseline eeg data\n",
    "    eeg_data_baseline = mne.io.read_raw_fif(f\"./data/RQ/topo_data/raw/eeg/{participant}_{snippet}_baseline_raw.fif\", verbose='ERROR')\n",
    "    eeg_data_baseline_array = mne.io.RawArray(eeg_data_baseline.get_data(), eeg_data_baseline.info, verbose='ERROR')\n",
    "\n",
    "    # Generate epochs\n",
    "    events_baseline = mne.make_fixed_length_events(eeg_data_baseline_array, id=1, duration=0.1)\n",
    "    epochs_baseline = mne.epochs.Epochs(eeg_data_baseline_array, events_baseline, event_id=1, tmin=0, tmax=3, baseline=None, preload=True, verbose='ERROR')\n",
    "\n",
    "    # Get channel data and scalings\n",
    "    ch_type = mne.channels._get_ch_type(epochs_baseline, None)\n",
    "    scalings = mne.defaults._handle_default('scalings', None)\n",
    "    scaling = scalings[ch_type]\n",
    "\n",
    "    # Generate psd\n",
    "    psd_baseline, freq_baseline = mne.time_frequency.psd_multitaper(epochs_baseline, verbose='ERROR')\n",
    "    psd_baseline_average = psd_baseline.mean(axis=0)\n",
    "    psd_baseline_average *= scaling**2\n",
    "\n",
    "    # Convert values to decibel\n",
    "    for x in range(0, psd_baseline_average.shape[0]):\n",
    "        for y in range(0, psd_baseline_average.shape[1]):\n",
    "            psd_baseline_average[x,y] = _to_decibel(psd_baseline_average[x,y])\n",
    "\n",
    "    # Save psd data\n",
    "    np.savetxt(f\"./data/RQ/topo_data/raw/psds/{participant}_{snippet}_baseline.csv\", psd_baseline_average, delimiter=\";\")\n",
    "\n",
    "    # Free memory\n",
    "    del epochs_baseline\n",
    "    del events_baseline\n",
    "    del eeg_data_baseline_array\n",
    "    del eeg_data_baseline\n",
    "\n",
    "    # Read program eeg data\n",
    "    eeg_data_program = mne.io.read_raw_fif(f\"./data/RQ/topo_data/raw/eeg/{participant}_{snippet}_program_raw.fif\", verbose='ERROR')\n",
    "    eeg_data_program_array = mne.io.RawArray(eeg_data_program.get_data(), eeg_data_program.info, verbose='ERROR')\n",
    "\n",
    "    # Generate epochs\n",
    "    events_program = mne.make_fixed_length_events(eeg_data_program_array, id=1, duration=0.1)\n",
    "    epochs_program = mne.epochs.Epochs(eeg_data_program_array, events_program, event_id=1, tmin=0, tmax=3, baseline=None, preload=True, verbose='ERROR')\n",
    "\n",
    "    # Get channel data and scalings\n",
    "    ch_type = mne.channels._get_ch_type(epochs_program, None)\n",
    "    scalings = mne.defaults._handle_default('scalings', None)\n",
    "    scaling = scalings[ch_type]\n",
    "\n",
    "    # Generate psd\n",
    "    psd_program, freq_program = mne.time_frequency.psd_multitaper(epochs_program, verbose='ERROR')\n",
    "    psd_program_average = psd_program.mean(axis=0)\n",
    "    psd_program_average *= scaling**2\n",
    "\n",
    "    # Convert values to decibel\n",
    "    for x in range(0, psd_program_average.shape[0]):\n",
    "        for y in range(0, psd_program_average.shape[1]):\n",
    "            psd_program_average[x,y] = _to_decibel(psd_program_average[x,y])\n",
    "\n",
    "    # Save psd data\n",
    "    np.savetxt(f\"./data/RQ/topo_data/raw/psds/{participant}_{snippet}_program.csv\", psd_program_average, delimiter=\";\")\n",
    "\n",
    "    # Save info data and freq data for later use (equally for all tasks)\n",
    "    if not os.path.exists(\"./data/RQ/topo_data/info-epo.fif\"):\n",
    "        epochs_program.save(f\"./data/RQ/topo_data/info-epo.fif\", overwrite=True)\n",
    "    if not os.path.exists(\"./data/RQ/topo_data/freq.csv\"):\n",
    "        np.savetxt(f\"./data/RQ/topo_data/freq.csv\", freq_program, delimiter=\";\")\n",
    "\n",
    "    # Free memory\n",
    "    del epochs_program\n",
    "    del events_program\n",
    "    del eeg_data_program_array\n",
    "    del eeg_data_program\n",
    "\n",
    "    # Calculate psd for program comprehension minus baseline data\n",
    "    psd_program_minus_baseline_average = np.subtract(psd_program_average, psd_baseline_average)\n",
    "\n",
    "    # Save psd data\n",
    "    np.savetxt(f\"./data/RQ/topo_data/raw/psds/{participant}_{snippet}_program_minus_baseline.csv\", psd_program_minus_baseline_average, delimiter=\";\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Merge PSDS per baseline per participant"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/topo_data/raw_participant\"):\n",
    "    os.makedirs(\"./data/RQ/topo_data/raw_participant\")\n",
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/topo_data/raw_all\"):\n",
    "    os.makedirs(\"./data/RQ/topo_data/raw_all\")\n",
    "\n",
    "for baseline in tqdm(df_topo[\"Baseline\"].unique()):\n",
    "    df_baseline = df_topo[df_topo[\"Baseline\"] == baseline]\n",
    "\n",
    "    for participant in tqdm(df_baseline[\"Participant\"].unique()):\n",
    "        df_participant = df_baseline[df_baseline[\"Participant\"] == participant]\n",
    "\n",
    "        # Init dataframes for participant psd data\n",
    "        df_baseline_data_participant = pd.DataFrame(columns=columns_with_channel)\n",
    "        df_program_data_participant = pd.DataFrame(columns=columns_with_channel)\n",
    "        df_program_minus_baseline_data_participant = pd.DataFrame(columns=columns_with_channel)\n",
    "\n",
    "        for index, row in df_participant.iterrows():\n",
    "            snippet = row[\"Algorithm\"]\n",
    "\n",
    "            # Load psd data\n",
    "            baseline_psds = np.loadtxt(f\"./data/RQ/topo_data/raw/psds/{participant}_{snippet}_baseline.csv\", delimiter=\";\")\n",
    "            program_psds = np.loadtxt(f\"./data/RQ/topo_data/raw/psds/{participant}_{snippet}_program.csv\", delimiter=\";\")\n",
    "            program_minus_baseline_psds = np.loadtxt(f\"./data/RQ/topo_data/raw/psds/{participant}_{snippet}_program_minus_baseline.csv\", delimiter=\";\")\n",
    "\n",
    "            channel_number = 1\n",
    "            for channel in baseline_psds:\n",
    "                # Concat channel number and baseline psd data for channel\n",
    "                baseline_data = [channel_number]\n",
    "                baseline_data.extend(channel)\n",
    "\n",
    "                # Add channel psd data to dataframes\n",
    "                df_baseline_data_participant = pd.concat([df_baseline_data_participant, pd.DataFrame(data=[baseline_data], columns=columns_with_channel)],ignore_index=True)\n",
    "                channel_number += 1\n",
    "\n",
    "            channel_number = 1\n",
    "            for channel in program_psds:\n",
    "                # Concat channel number and program psd data for channel\n",
    "                program_data = [channel_number]\n",
    "                program_data.extend(channel)\n",
    "\n",
    "                # Add channel psd data to dataframes\n",
    "                df_program_data_participant = pd.concat([df_program_data_participant, pd.DataFrame(data=[program_data], columns=columns_with_channel)],ignore_index=True)\n",
    "                channel_number += 1\n",
    "\n",
    "            channel_number = 1\n",
    "            for channel in program_minus_baseline_psds:\n",
    "                # Concat channel number and program psd data for channel\n",
    "                program_minus_baseline_data = [channel_number]\n",
    "                program_minus_baseline_data.extend(channel)\n",
    "\n",
    "                # Add channel psd data to dataframes\n",
    "                df_program_minus_baseline_data_participant = pd.concat([df_program_minus_baseline_data_participant, pd.DataFrame(data=[program_minus_baseline_data], columns=columns_with_channel)],ignore_index=True)\n",
    "                channel_number += 1\n",
    "\n",
    "        # Save baseline psd data for participant\n",
    "        df_baseline_data_participant = df_baseline_data_participant.groupby('Channel')[columns_with_channel[1:]].agg('mean')\n",
    "        baseline_data_participant = df_baseline_data_participant.to_numpy()\n",
    "        np.savetxt(f\"./data/RQ/topo_data/raw_participant/{participant}_{baseline}_baseline.csv\", baseline_data_participant, delimiter=\";\")\n",
    "\n",
    "        # Save program psd data for participant\n",
    "        df_program_data_participant = df_program_data_participant.groupby('Channel')[columns_with_channel[1:]].agg('mean')\n",
    "        program_data_participant = df_program_data_participant.to_numpy()\n",
    "        np.savetxt(f\"./data/RQ/topo_data/raw_participant/{participant}_{baseline}_program.csv\", program_data_participant, delimiter=\";\")\n",
    "\n",
    "        # Save program minus baseline psd data for participant\n",
    "        df_program_minus_baseline_data_participant = df_program_minus_baseline_data_participant.groupby('Channel')[columns_with_channel[1:]].agg('mean')\n",
    "        program_minus_baseline_data_participant = df_program_minus_baseline_data_participant.to_numpy()\n",
    "        np.savetxt(f\"./data/RQ/topo_data/raw_participant/{participant}_{baseline}_program_minus_baseline.csv\", program_minus_baseline_data_participant, delimiter=\";\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PSDS Outlier detection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/psds\"):\n",
    "    os.makedirs(\"./data/RQ/psds\")\n",
    "if not os.path.exists(\"./data/RQ/psds/participant\"):\n",
    "    os.makedirs(\"./data/RQ/psds/participant\")\n",
    "\n",
    "df_dropped_channel = pd.DataFrame(columns=[\"Participant\", \"Baseline\", \"Type\", \"Channel\"])\n",
    "\n",
    "for participant in tqdm(df_topo[\"Participant\"].unique()):\n",
    "    df_participant = df_topo[df_topo[\"Participant\"] == participant]\n",
    "\n",
    "    if participant != 5:\n",
    "        continue\n",
    "\n",
    "    for baseline in df_participant[\"Baseline\"].unique():\n",
    "        df_baseline = df_participant[df_participant[\"Baseline\"] == baseline]\n",
    "\n",
    "        # Init dataframes for participant psd data\n",
    "        df_baseline_data_participant = pd.read_csv(f\"./data/RQ/topo_data/raw_participant/{participant}_{baseline}_baseline.csv\", delimiter=\";\", names=columns)\n",
    "        df_baseline_data_participant = df_baseline_data_participant.set_index(np.arange(1,65,1))\n",
    "        df_program_data_participant = pd.read_csv(f\"./data/RQ/topo_data/raw_participant/{participant}_{baseline}_program.csv\", delimiter=\";\", names=columns)\n",
    "        df_program_data_participant = df_program_data_participant.set_index(np.arange(1,65,1))\n",
    "\n",
    "        df_psd_data_baseline = pd.DataFrame(columns=[\"Channel\", \"PSD\"])\n",
    "        df_psd_data_program = pd.DataFrame(columns=[\"Channel\", \"PSD\"])\n",
    "\n",
    "        all_psd_baseline = []\n",
    "        all_psd_program = []\n",
    "\n",
    "        for row_index, row in df_baseline_data_participant.iterrows():\n",
    "            for col_index, value in row.iteritems():\n",
    "                df_psd_data_baseline = pd.concat([df_psd_data_baseline, pd.DataFrame(data=[{\"Channel\": row_index, \"PSD\": value}])],ignore_index=True)\n",
    "                all_psd_baseline.append(value)\n",
    "\n",
    "        for row_index, row in df_program_data_participant.iterrows():\n",
    "            for col_index, value in row.iteritems():\n",
    "                df_psd_data_program = pd.concat([df_psd_data_program, pd.DataFrame(data=[{\"Channel\": row_index, \"PSD\": value}])],ignore_index=True)\n",
    "                all_psd_program.append(value)\n",
    "\n",
    "        # Calculate interquartile range for task\n",
    "        q1_baseline, q2_baseline, q3_baseline = np.percentile(all_psd_baseline, [25,50,75])\n",
    "        minimum_baseline = q1_baseline - 1.5*(q3_baseline-q1_baseline)\n",
    "        maximum_baseline = q3_baseline + 1.5*(q3_baseline-q1_baseline)\n",
    "\n",
    "        print(baseline)\n",
    "        print(\"Baseline\")\n",
    "        print(f\"Q1: {q1_baseline}\")\n",
    "        print(f\"Q3: {q3_baseline}\")\n",
    "        print(f\"Mean: {np.mean(all_psd_baseline)}\")\n",
    "        print(f\"Max: {np.max(all_psd_baseline)}\")\n",
    "\n",
    "        q1_program, q2_program, q3_program = np.percentile(all_psd_program, [25,50,75])\n",
    "        minimum_program = q1_program - 1.5*(q3_program-q1_program)\n",
    "        maximum_program = q3_program + 1.5*(q3_program-q1_program)\n",
    "\n",
    "        print(\"Program\")\n",
    "        print(f\"Q1: {q1_program}\")\n",
    "        print(f\"Q3: {q3_program}\")\n",
    "        print(f\"Mean: {np.mean(all_psd_program)}\")\n",
    "        print(f\"Max: {np.max(all_psd_program)}\")\n",
    "\n",
    "        outlier_per_channel_baseline = dict(zip(np.arange(1,65,1),np.zeros(64)))\n",
    "        outlier_per_channel_program = dict(zip(np.arange(1,65,1),np.zeros(64)))\n",
    "\n",
    "        for index, row in df_psd_data_baseline.iterrows():\n",
    "            if row[\"PSD\"] < minimum_baseline or row[\"PSD\"] > maximum_baseline:\n",
    "                outlier_per_channel_baseline[row[\"Channel\"]] += 1\n",
    "\n",
    "        df_outlier_per_channel_baseline_save = pd.DataFrame(data=outlier_per_channel_baseline, index=[0])\n",
    "        df_outlier_per_channel_baseline_save.to_csv(f\"./data/RQ/psds/participant/{participant}_{baseline}_outlier_per_channel_baseline.csv\", sep=\";\")\n",
    "\n",
    "        for index, row in df_psd_data_program.iterrows():\n",
    "            if row[\"PSD\"] < minimum_program or row[\"PSD\"] > maximum_program:\n",
    "                outlier_per_channel_program[row[\"Channel\"]] += 1\n",
    "\n",
    "        df_outlier_per_channel_program_save = pd.DataFrame(data=outlier_per_channel_program, index=[0])\n",
    "        df_outlier_per_channel_program_save.to_csv(f\"./data/RQ/psds/participant/{participant}_{baseline}_outlier_per_channel_program.csv\", sep=\";\")\n",
    "\n",
    "        for key, value in outlier_per_channel_baseline.items():\n",
    "            if value > channel_len/2:\n",
    "                df_dropped_channel = pd.concat([df_dropped_channel, pd.DataFrame(data=[{\"Participant\": participant, \"Baseline\": baseline ,\"Type\" : \"Baseline\", \"Channel\": key}])],ignore_index=True)\n",
    "\n",
    "        for key, value in outlier_per_channel_program.items():\n",
    "            if value > channel_len/2:\n",
    "                df_dropped_channel = pd.concat([df_dropped_channel, pd.DataFrame(data=[{\"Participant\": participant, \"Baseline\": baseline, \"Type\" : \"Program\", \"Channel\": key}])],ignore_index=True)\n",
    "\n",
    "df_dropped_channel.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "# Save dataframe to file\n",
    "df_dropped_channel.to_csv(\"./data/RQ/psds/dropped_channel.csv\", sep=\";\", index=False)\n",
    "df_dropped_channel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Drop PSD outlier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./data/RQ/topo_data/filtered_participant\"):\n",
    "    os.makedirs(\"./data/RQ/topo_data/filtered_participant\")\n",
    "\n",
    "# Copy files to filtered data\n",
    "raw_participant_folder = \"./data/RQ/topo_data/raw_participant/\"\n",
    "filtered_participant_folder = \"./data/RQ/topo_data/filtered_participant/\"\n",
    "\n",
    "# fetch all files\n",
    "for file_name in os.listdir(raw_participant_folder):\n",
    "    # construct full file path\n",
    "    source = raw_participant_folder + file_name\n",
    "    destination = filtered_participant_folder + file_name\n",
    "    # copy only files\n",
    "    if os.path.isfile(source):\n",
    "        shutil.copy(source, destination)\n",
    "\n",
    "for participant in tqdm(df_dropped_channel[\"Participant\"].unique()):\n",
    "    df_participant = df_dropped_channel[df_dropped_channel[\"Participant\"] == participant]\n",
    "\n",
    "    for baseline in df_participant[\"Baseline\"].unique():\n",
    "        df_baseline = df_participant[df_participant[\"Baseline\"] == baseline]\n",
    "\n",
    "        # Init dataframes for participant psd data\n",
    "        df_baseline_data_participant = pd.read_csv(f\"./data/RQ/topo_data/filtered_participant/{participant}_{baseline}_baseline.csv\", delimiter=\";\", names=columns)\n",
    "        df_baseline_data_participant = df_baseline_data_participant.set_index(np.arange(1,65,1))\n",
    "        df_program_data_participant = pd.read_csv(f\"./data/RQ/topo_data/filtered_participant/{participant}_{baseline}_program.csv\", delimiter=\";\", names=columns)\n",
    "        df_program_data_participant = df_program_data_participant.set_index(np.arange(1,65,1))\n",
    "        df_program_minus_baseline_data_participant = pd.read_csv(f\"./data/RQ/topo_data/filtered_participant/{participant}_{baseline}_program_minus_baseline.csv\", delimiter=\";\", names=columns)\n",
    "        df_program_minus_baseline_data_participant = df_program_minus_baseline_data_participant.set_index(np.arange(1,65,1))\n",
    "\n",
    "        for index, row in df_baseline.iterrows():\n",
    "            if row[\"Type\"] == \"Baseline\":\n",
    "                df_baseline_data_participant.loc[row[\"Channel\"], columns] = np.nan\n",
    "                df_program_minus_baseline_data_participant.loc[row[\"Channel\"], columns] = np.nan\n",
    "            else:\n",
    "                df_program_data_participant.loc[row[\"Channel\"], columns] = np.nan\n",
    "                df_program_minus_baseline_data_participant.loc[row[\"Channel\"], columns] = np.nan\n",
    "\n",
    "        # Save baseline psd data for participant\n",
    "        baseline_data_participant = df_baseline_data_participant.to_numpy()\n",
    "        np.savetxt(f\"./data/RQ/topo_data/filtered_participant/{participant}_{baseline}_baseline.csv\", baseline_data_participant, delimiter=\";\")\n",
    "\n",
    "        # Save program psd data for participant\n",
    "        program_data_participant = df_program_data_participant.to_numpy()\n",
    "        np.savetxt(f\"./data/RQ/topo_data/filtered_participant/{participant}_{baseline}_program.csv\", program_data_participant, delimiter=\";\")\n",
    "\n",
    "        # Save program minus baseline psd data for participant\n",
    "        program_minus_baseline_data_participant = df_program_minus_baseline_data_participant.to_numpy()\n",
    "        np.savetxt(f\"./data/RQ/topo_data/filtered_participant/{participant}_{baseline}_program_minus_baseline.csv\", program_minus_baseline_data_participant, delimiter=\";\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Merge PSDS per Baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/topo_data/all\"):\n",
    "    os.makedirs(\"./data/RQ/topo_data/all\")\n",
    "\n",
    "for baseline in tqdm(df_topo[\"Baseline\"].unique()):\n",
    "    df_baseline = df_topo[df_topo[\"Baseline\"] == baseline]\n",
    "\n",
    "    # Init dataframes for baseline psd data\n",
    "    df_baseline_data_all = pd.DataFrame(columns=columns_with_channel)\n",
    "    df_program_data_all = pd.DataFrame(columns=columns_with_channel)\n",
    "    df_program_minus_baseline_data_all = pd.DataFrame(columns=columns_with_channel)\n",
    "\n",
    "    for participant in tqdm(df_baseline[\"Participant\"].unique()):\n",
    "        # Load psd data\n",
    "        baseline_psds = np.loadtxt(f\"./data/RQ/topo_data/filtered_participant/{participant}_{baseline}_baseline.csv\", delimiter=\";\")\n",
    "        program_psds = np.loadtxt(f\"./data/RQ/topo_data/filtered_participant/{participant}_{baseline}_program.csv\", delimiter=\";\")\n",
    "        program_minus_baseline_psds = np.loadtxt(f\"./data/RQ/topo_data/filtered_participant/{participant}_{baseline}_program_minus_baseline.csv\", delimiter=\";\")\n",
    "\n",
    "        channel_number = 1\n",
    "        for channel in baseline_psds:\n",
    "            # Concat channel number and baseline psd data for channel\n",
    "            baseline_data = [channel_number]\n",
    "            baseline_data.extend(channel)\n",
    "\n",
    "            # Add channel psd data to dataframes\n",
    "            df_baseline_data_all = pd.concat([df_baseline_data_all, pd.DataFrame(data=[baseline_data], columns=columns_with_channel)],ignore_index=True)\n",
    "            channel_number += 1\n",
    "\n",
    "        channel_number = 1\n",
    "        for channel in program_psds:\n",
    "            # Concat channel number and program psd data for channel\n",
    "            program_data = [channel_number]\n",
    "            program_data.extend(channel)\n",
    "\n",
    "            # Add channel psd data to dataframes\n",
    "            df_program_data_all = pd.concat([df_program_data_all, pd.DataFrame(data=[program_data], columns=columns_with_channel)],ignore_index=True)\n",
    "            channel_number += 1\n",
    "\n",
    "        channel_number = 1\n",
    "        for channel in program_minus_baseline_psds:\n",
    "            # Concat channel number and program psd data for channel\n",
    "            program_minus_baseline_data = [channel_number]\n",
    "            program_minus_baseline_data.extend(channel)\n",
    "\n",
    "            # Add channel psd data to dataframes\n",
    "            df_program_minus_baseline_data_all = pd.concat([df_program_minus_baseline_data_all, pd.DataFrame(data=[program_minus_baseline_data], columns=columns_with_channel)],ignore_index=True)\n",
    "            channel_number += 1\n",
    "\n",
    "    # Save baseline psd data for all\n",
    "    df_baseline_data_all = df_baseline_data_all.groupby('Channel')[columns_with_channel[1:]].agg('mean')\n",
    "    baseline_data_all = df_baseline_data_all.to_numpy()\n",
    "    np.savetxt(f\"./data/RQ/topo_data/all/{baseline}_baseline.csv\", baseline_data_all, delimiter=\";\")\n",
    "\n",
    "    # Save program psd data for all\n",
    "    df_program_data_all = df_program_data_all.groupby('Channel')[columns_with_channel[1:]].agg('mean')\n",
    "    program_data_all = df_program_data_all.to_numpy()\n",
    "    np.savetxt(f\"./data/RQ/topo_data/all/{baseline}_program.csv\", program_data_all, delimiter=\";\")\n",
    "\n",
    "    # Save program minus baseline psd data for all\n",
    "    df_program_minus_baseline_data_all = df_program_minus_baseline_data_all.groupby('Channel')[columns_with_channel[1:]].agg('mean')\n",
    "    program_minus_baseline_data_all = df_program_minus_baseline_data_all.to_numpy()\n",
    "    np.savetxt(f\"./data/RQ/topo_data/all/{baseline}_program_minus_baseline.csv\", program_minus_baseline_data_all, delimiter=\";\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot topoplots per baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create folder if not exists\n",
    "if not os.path.exists(\"./data/RQ/plots/topoplots/total/raw\"):\n",
    "    os.makedirs(\"./data/RQ/plots/topoplots/total/raw\")\n",
    "if not os.path.exists(\"./data/RQ/plots/topoplots/total/program_with_baseline\"):\n",
    "    os.makedirs(\"./data/RQ/plots/topoplots/total/program_with_baseline\")\n",
    "\n",
    "# Load info and freq data\n",
    "epochs_data = mne.read_epochs(f\"./data/RQ/topo_data/info-epo.fif\", preload=True, verbose=\"ERROR\")\n",
    "info_data = epochs_data.info\n",
    "freq_data = np.loadtxt(f\"./data/RQ/topo_data/freq.csv\", delimiter=\";\")\n",
    "\n",
    "# Get channel data and unit\n",
    "ch_type = mne.channels._get_ch_type(epochs_data, None)\n",
    "units = mne.defaults._handle_default('units', None)\n",
    "unit = units[ch_type]\n",
    "\n",
    "# Init total figures per participant\n",
    "all_topo_baseline_fig, all_topo_baseline_axs = plt.subplots(5, 4, sharex='all', sharey='all',figsize=(15,12))\n",
    "all_topo_program_fig, all_topo_program_axs = plt.subplots(5, 4, sharex='all', sharey='all',figsize=(15,12))\n",
    "all_topo_program_with_baseline_fig, all_topo_program_with_baseline_axs = plt.subplots(5, 4, sharex='all', sharey='all',figsize=(15,12))\n",
    "\n",
    "all_topo_paper_fig, all_topo_paper_axs = plt.subplots(3, 4, sharex='all', sharey='all',figsize=(16,9))\n",
    "\n",
    "# Row label\n",
    "rows = ['A', 'B', 'C']\n",
    "for ax, row in zip(all_topo_paper_axs[:,0], rows):\n",
    "    ax.annotate(row, xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - 5, 0), xycoords=ax.yaxis.label, textcoords='offset points', fontsize=20, ha='right', va='center')\n",
    "\n",
    "count = 0\n",
    "\n",
    "for baseline in tqdm(sorted(df_topo[\"Baseline\"].unique())):\n",
    "\n",
    "    # Get axs from total figures\n",
    "    axs_all_baseline = [all_topo_baseline_axs[0,count%4], all_topo_baseline_axs[1,count%4], all_topo_baseline_axs[2,count%4], all_topo_baseline_axs[3,count%4], all_topo_baseline_axs[4,count%4]]\n",
    "    axs_all_program = [all_topo_program_axs[0,count%4], all_topo_program_axs[1,count%4], all_topo_program_axs[2,count%4], all_topo_program_axs[3,count%4], all_topo_program_axs[4,count%4]]\n",
    "    axs_all_program_with_baseline = [all_topo_program_with_baseline_axs[0,count%4], all_topo_program_with_baseline_axs[1,count%4], all_topo_program_with_baseline_axs[2,count%4], all_topo_program_with_baseline_axs[3,count%4], all_topo_program_with_baseline_axs[4,count%4]]\n",
    "    axs_paper_without_baseline = [all_topo_paper_axs[0,count%4]]\n",
    "    axs_paper_with_baseline = [all_topo_paper_axs[1,count%4]]\n",
    "    axs_paper_with_baseline_theta = [all_topo_paper_axs[2,count%4]]\n",
    "\n",
    "    # Load baseline psds\n",
    "    psd_baseline_average = np.loadtxt(f\"./data/RQ/topo_data/all/{baseline}_baseline.csv\", delimiter=\";\")\n",
    "\n",
    "    # Create total topoplot\n",
    "    mne.viz.topomap.plot_psds_topomap(psd_baseline_average, freq_data, info_data, bands=[(4, 8, f'{translate(baseline)} Theta'), (8, 12, f'{translate(baseline)} Alpha'), (12, 30, f'{translate(baseline)} Beta'), (30, 45, f'{translate(baseline)} Gamma'), (0, 45, f'{translate(baseline)} All')], show=False, axes=axs_all_baseline, cmap=\"jet\", dB=False, unit=unit, vlim=(0,25))\n",
    "\n",
    "    # Init figure for single topoplot\n",
    "    fig = plt.figure(tight_layout=True, figsize=(15, 3))\n",
    "\n",
    "    # Create subfigures for single topoplot\n",
    "    subfigure = fig.subfigures(nrows=1, ncols=1)\n",
    "    axs = subfigure.subplots(nrows=1, ncols=5)\n",
    "\n",
    "    # Create single topoplot\n",
    "    mne.viz.topomap.plot_psds_topomap(psd_baseline_average, freq_data, info_data, bands=[(4, 8, f'Theta'), (8, 12, f'Alpha'), (12, 30, f'Beta'), (30, 45, f'Gamma'), (0, 45, f'All')], show=False, axes=axs, cmap=\"jet\", dB=False, unit=unit, vlim=(0,25))\n",
    "\n",
    "    # Save single topoplot\n",
    "    plt.savefig(f\"./data/RQ/plots/topoplots/total/raw/{translate(baseline)}_baseline_topo.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Load program psds\n",
    "    psd_program_average = np.loadtxt(f\"./data/RQ/topo_data/all/{baseline}_program.csv\", delimiter=\";\")\n",
    "\n",
    "    # Create total topoplot\n",
    "    mne.viz.topomap.plot_psds_topomap(psd_program_average, freq_data, info_data, bands=[(4, 8, f'$ProgCompr_{{{translate(baseline)}}}$ Theta'), (8, 12, f'$ProgCompr_{{{translate(baseline)}}}$ Alpha'), (12, 30, f'$ProgCompr_{{{translate(baseline)}}}$ Beta'), (30, 45, f'$ProgCompr_{{{translate(baseline)}}}$ Gamma'), (0, 45, f'$ProgCompr_{{{translate(baseline)}}}$ All')], show=False, axes=axs_all_program, cmap=\"jet\", dB=False, unit=unit, vlim=(0,25))\n",
    "\n",
    "    # Create paper topoplot\n",
    "    mne.viz.topomap.plot_psds_topomap(psd_program_average, freq_data, info_data, bands=[(0, 45, f'$ProgCompr_{{{translate(baseline)}}}$')], show=False, axes=axs_paper_without_baseline, cmap=\"jet\", dB=False, unit=unit, vlim=(0,25))\n",
    "\n",
    "    # Init figure for single topoplot\n",
    "    fig = plt.figure(tight_layout=True, figsize=(15, 3))\n",
    "\n",
    "    # Create subfigures for single topoplot\n",
    "    subfigure = fig.subfigures(nrows=1, ncols=1)\n",
    "    axs = subfigure.subplots(nrows=1, ncols=5)\n",
    "\n",
    "    # Create single topoplot\n",
    "    mne.viz.topomap.plot_psds_topomap(psd_program_average, freq_data, info_data, bands=[(4, 8, f'Theta'), (8, 12, f'Alpha'), (12, 30, f'Beta'), (30, 45, f'Gamma'), (0, 45, f'All')], show=False, axes=axs, cmap=\"jet\", dB=False, unit=unit, vlim=(0,25))\n",
    "\n",
    "    # Save single topoplot\n",
    "    plt.savefig(f\"./data/RQ/plots/topoplots/total/raw/{translate(baseline)}_program_topo.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Calculate program minus baseline psds\n",
    "    psd_program_minus_baseline_average = np.loadtxt(f\"./data/RQ/topo_data/all/{baseline}_program_minus_baseline.csv\", delimiter=\";\")\n",
    "\n",
    "    # Create total topoplot\n",
    "    mne.viz.topomap.plot_psds_topomap(psd_program_minus_baseline_average, freq_data, info_data, bands=[(4, 8, f'$ProgCompr_{{{translate(baseline)}}}$ > {translate(baseline)} Theta'), (8, 12, f'$ProgCompr_{{{translate(baseline)}}}$ > {translate(baseline)} Alpha'), (12, 30, f'$ProgCompr_{{{translate(baseline)}}}$ > {translate(baseline)} Beta'), (30, 45, f'$ProgCompr_{{{translate(baseline)}}}$ > {translate(baseline)} Gamma'), (0, 45, f'$ProgCompr_{{{translate(baseline)}}}$ > {translate(baseline)} All')], cmap='jet', axes=axs_all_program_with_baseline, show=False, dB=False, unit=unit, vlim=(-3,3))\n",
    "\n",
    "    # Create paper topoplot\n",
    "    mne.viz.topomap.plot_psds_topomap(psd_program_minus_baseline_average, freq_data, info_data, bands=[(0, 45, f'$ProgCompr_{{{translate(baseline)}}}$ > {translate(baseline)}')], show=False, axes=axs_paper_with_baseline, cmap=\"jet\", dB=False, unit=unit, vlim=(-3,3))\n",
    "    mne.viz.topomap.plot_psds_topomap(psd_program_minus_baseline_average, freq_data, info_data, bands=[(4, 8, f'$ProgCompr_{{{translate(baseline)}}}$ > {translate(baseline)} Theta')], show=False, axes=axs_paper_with_baseline_theta, cmap=\"jet\", dB=False, unit=unit, vlim=(-3,3))\n",
    "\n",
    "    # Init figure for single topoplot\n",
    "    fig = plt.figure(tight_layout=True, figsize=(15, 3))\n",
    "\n",
    "    # Create subfigures for single topoplot\n",
    "    subfigure = fig.subfigures(nrows=1, ncols=1)\n",
    "    axs = subfigure.subplots(nrows=1, ncols=5)\n",
    "\n",
    "    # Create single topoplot\n",
    "    mne.viz.topomap.plot_psds_topomap(psd_program_minus_baseline_average, freq_data, info_data, bands=[(4, 8, f'Theta'), (8, 12, f'Alpha'), (12, 30, f'Beta'), (30, 45, f'Gamma'), (0, 45, f'All')], cmap='jet', axes=axs, show=False, dB=False, unit=unit, vlim=(-3,3))\n",
    "\n",
    "    # Save single topoplot\n",
    "    plt.savefig(f\"./data/RQ/plots/topoplots/total/program_with_baseline/{translate(baseline)}_topo.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    axs_paper_without_baseline[0].title.set_size(15)\n",
    "    axs_paper_with_baseline[0].title.set_size(15)\n",
    "    axs_paper_with_baseline_theta[0].title.set_size(15)\n",
    "\n",
    "\n",
    "    count += 1\n",
    "\n",
    "# Save total plots\n",
    "plt.figure(all_topo_baseline_fig)\n",
    "plt.savefig(f\"./data/RQ/plots/topoplots/total/raw/all_baseline_topo.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(all_topo_program_fig)\n",
    "plt.savefig(f\"./data/RQ/plots/topoplots/total/raw/all_program_topo.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(all_topo_program_with_baseline_fig)\n",
    "plt.savefig(f\"./data/RQ/plots/topoplots/total/program_with_baseline/all_topo.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "for ax in all_topo_paper_fig.get_axes():\n",
    "    ax.xaxis.label.set_size(12)\n",
    "    ax.yaxis.label.set_size(12)\n",
    "    try:\n",
    "        cb = ax.images[-1].colorbar\n",
    "        ticklabs = cb.ax.get_yticklabels()\n",
    "        cb.ax.set_yticklabels(ticklabs, fontsize=12)\n",
    "    except:\n",
    "        pass\n",
    "plt.figure(all_topo_paper_fig)\n",
    "plt.savefig(f\"./data/RQ/plots/topoplots/total/topo_paper.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
